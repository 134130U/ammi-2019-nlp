{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ken_lm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/ken_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yZ7xgQ8TjEw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CVGBphEakB1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# KenLM Framework for Language Modeling"
      ]
    },
    {
      "metadata": {
        "id": "_mu9NmEikGja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Install KenLM**\n",
        "\n",
        "Download stable release and unzip: http://kheafield.com/code/kenlm.tar.gz\n",
        "\n",
        "Need Boost >= 1.42.0 and bjam\n",
        "*   Ubuntu: sudo apt-get install libboost-all-dev\n",
        "*   Mac: brew install boost; brew install bjam\n",
        "\n",
        "Run within kenlm directory:\n",
        "    \n",
        "*  mkdir -p build\n",
        "  *  cd build\n",
        "  *  cmake ..\n",
        "  *  make -j 4\n",
        " \n",
        "pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "\n",
        "For more information on KenLM see: https://github.com/kpu/kenlm and http://kheafield.com/code/kenlm/\n",
        "\n",
        "TODO: need to get some trained language models: nli_5gram.binary and sentiment.binary. How should we do that? \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "90dlx6MzkrRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "6d2e42e5-3675-4d87-b79a-ff562b9e4c27"
      },
      "cell_type": "code",
      "source": [
        "import kenlm\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-32377a350081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kenlm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xX9dfZJxlmi6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the models"
      ]
    },
    {
      "metadata": {
        "id": "HHFdQBJClI6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "f23450f4-5396-4c3f-a8b1-6dc7ec434360"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "nli_model = kenlm.Model(\"nli_5gram.binary\") # Or correct path to binary\n",
        "sent_model =  kenlm.Model(\"sentiment.binary\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a3b4ba26443d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnli_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nli_5gram.binary\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Or correct path to binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msent_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mkenlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment.binary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'kenlm' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ov42EMhflktI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The KenLM model reports negative log likelihood, not perplexity. So we'll be converting the score and report net perplexity. The following function calculate the perpelxity, get_ppl, and find all OOV words, get_oov.\n",
        "\n",
        "Pereplexity is defined as follows, $$ PPL = b^{- \\frac{1}{N} \\sum_{i=1}^N \\log_b q(x_i)} $$ All probabilities here are in log base 10 so to convert to perplexity, we do the following $$PPL = 10^{-\\log(P) / N} $$ where $P$ is the total NLL, and $N$ is the word count."
      ]
    },
    {
      "metadata": {
        "id": "KLsISNQNlKff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_ppl(lm, sentences):\n",
        "    \"\"\"\n",
        "    Assume sentences is a list of strings (space delimited sentences)\n",
        "    \"\"\"\n",
        "    total_nll = 0\n",
        "    total_wc = 0\n",
        "    for sent in sentences:\n",
        "        sent = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", sent)\n",
        "        words = sent.strip().split()\n",
        "        score = lm.score(sent, bos=False, eos=False)\n",
        "        word_count = len(words)\n",
        "        total_wc += word_count\n",
        "        total_nll += score\n",
        "    ppl = 10**-(total_nll/total_wc)\n",
        "    return ppl\n",
        "\n",
        "def get_oov(model, data):\n",
        "    oov = []\n",
        "    vocab = []\n",
        "    for sent in data:\n",
        "        sentence = sent\n",
        "        words =  sentence.split()\n",
        "        vocab += words\n",
        "        # Find out-of-vocabulary words\n",
        "        for w in words:\n",
        "            if w not in model:\n",
        "                    oov.append(w)\n",
        "    return set(oov), set(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4unImHqblPQ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for loading the data"
      ]
    },
    {
      "metadata": {
        "id": "7ln_ECRFlNnA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f): \n",
        "            data.append(line)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rutA7273lTg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To start to get a feel for how the model performs on different sentences, try out different sentences below. (Note that this score isn't the perplexity but the negative log likelihood, you can convert it to perplexity as we did before)"
      ]
    },
    {
      "metadata": {
        "id": "7PswkSPllUwB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = \"I am a chimpanzee .\"\n",
        "print(nli_model.score(sentence))\n",
        "print(sent_model.score(sentence))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}