{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/ken_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVGBphEakB1c"
   },
   "source": [
    "# KenLM Framework for Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mu9NmEikGja"
   },
   "source": [
    "\n",
    "**Install KenLM**\n",
    "\n",
    "Download stable release and unzip: http://kheafield.com/code/kenlm.tar.gz\n",
    "\n",
    "Need Boost >= 1.42.0 and bjam\n",
    "*   Ubuntu: sudo apt-get install libboost-all-dev\n",
    "*   Mac: brew install boost; brew install bjam\n",
    "\n",
    "Run within kenlm directory:\n",
    "    \n",
    "*  mkdir -p build\n",
    "  *  cd build\n",
    "  *  cmake ..\n",
    "  *  make -j 4\n",
    " \n",
    "pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "\n",
    "For more information on KenLM see: https://github.com/kpu/kenlm and http://kheafield.com/code/kenlm/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "90dlx6MzkrRB",
    "outputId": "6d2e42e5-3675-4d87-b79a-ff562b9e4c27"
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import os\n",
    "import re\n",
    "import utils.ngram_utils as ngram_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/roberta/ammi-2019-nlp/data/'\n",
    "os.chdir(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-gram model with KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:860352 2:75230912512 3:141057966080\n",
      "Statistics:\n",
      "1 71696 D1=0.690098 D2=0.962667 D3+=1.22676\n",
      "2 1239185 D1=0.712943 D2=1.05296 D3+=1.36242\n",
      "3 4834597 D1=0.772513 D2=1.0869 D3+=1.33918\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 113 assuming -p 1.5\n",
      "probing 120 assuming -r models -p 1.5\n",
      "trie     44 without quantization\n",
      "trie     24 assuming -q 8 -b 8 quantization \n",
      "trie     42 assuming -a 22 array pointer compression\n",
      "trie     22 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:860352 2:19826960 3:96691940\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:860352 2:19826960 3:96691940\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Chain sizes: 1:860352 2:19826960 3:96691940\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:211453080 kB\tVmRSS:27916 kB\tRSSMax:0 kB\tuser:0\tsys:0\tCPU:0\treal:40.75\n"
     ]
    }
   ],
   "source": [
    "cat train.txt | /home/roberta/kenlm/bin/lmplz -o 3 > amazonLM3.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/roberta'\n",
    "# os.chdir(path)\n",
    "# !kenlm/bin/lmplz amazonLM.arpa amazonLM.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "model_3n = kenlm.LanguageModel('amazonLM3.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .txt files and create lists of reviews\n",
    "\n",
    "train_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/train.txt', 'r') as f:\n",
    "    train_data = [review for review in f.read().split('\\n') if review]\n",
    "    \n",
    "valid_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/valid.txt', 'r') as f:\n",
    "    valid_data = [review for review in f.read().split('\\n') if review]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3412260b1d8e49b7b1220748097849a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ad7bfa34be46d39ece0ca346397711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = ngram_utils.tokenize_dataset(train_data)\n",
    "valid_data_tokenized, all_tokens_valid = ngram_utils.tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a great tutu and at a really great price .',\n",
       " \"it doesn ' t look cheap at all .\",\n",
       " \"i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly .\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "for t in train_data_tokenized:\n",
    "    train_data.append(' '.join(t))\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good value .',\n",
       " 'not super cheap material .',\n",
       " 'at first , i was absolutely delighted with these peds . . .']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = []\n",
    "for t in valid_data_tokenized:\n",
    "    valid_data.append(' '.join(t))\n",
    "valid_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ov42EMhflktI"
   },
   "source": [
    "#### The KenLM model reports negative log likelihood, not perplexity. So we'll be converting the score and report net perplexity. The following function calculate the perpelxity, get_ppl, and find all OOV words, get_oov.\n",
    "\n",
    "#### Pereplexity is defined as follows, $$ PPL = b^{- \\frac{1}{N} \\sum_{i=1}^N \\log_b q(x_i)} $$ All probabilities here are in log base 10 so to convert to perplexity, we do the following $$PPL = 10^{-\\log(P) / N} $$ where $P$ is the total NLL, and $N$ is the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLsISNQNlKff"
   },
   "outputs": [],
   "source": [
    "def get_ppl(lm, sentences):\n",
    "    \"\"\"\n",
    "    Assume sentences is a list of strings (space delimited sentences)\n",
    "    \"\"\"\n",
    "    total_nll = 0\n",
    "    total_wc = 0\n",
    "    for sent in sentences:\n",
    "        sent = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", sent)\n",
    "        words = sent.strip().split()\n",
    "        score = lm.score(sent, bos=False, eos=False)\n",
    "        word_count = len(words)\n",
    "        total_wc += word_count\n",
    "        total_nll += score\n",
    "    ppl = 10**-(total_nll/total_wc)\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.08424638548333"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ppl = get_ppl(model_3n, train_data)\n",
    "train_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.55326951549414"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ppl = get_ppl(model_3n, valid_data)\n",
    "valid_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13199.934380820527"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i like pandas']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4unImHqblPQ9"
   },
   "source": [
    "Function for loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230.70962836087295"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i like this tutu']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557.8821726391418"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', '.']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.331202531779226"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['.']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3028.4422169553886"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['who wants dinner?']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.634355092812754"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i want to get a refund']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.70582444534253"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this watch is not what i expected']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.99315866512597"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this fits me perfectly .']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.37286419366077"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this coat fits me perfectly ?']\n",
    "ppl = get_ppl(model_3n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-gram model with KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:860352 2:21101352960 3:39565037568 4:63304056832 5:92318425088\n",
      "Statistics:\n",
      "1 71696 D1=0.690098 D2=0.962667 D3+=1.22676\n",
      "2 1239185 D1=0.712943 D2=1.05296 D3+=1.36242\n",
      "3 4834597 D1=0.796199 D2=1.09701 D3+=1.35908\n",
      "4 9215190 D1=0.868874 D2=1.16401 D3+=1.3733\n",
      "5 12376562 D1=0.898907 D2=1.2197 D3+=1.36975\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 564 assuming -p 1.5\n",
      "probing 651 assuming -r models -p 1.5\n",
      "trie    261 without quantization\n",
      "trie    142 assuming -q 8 -b 8 quantization \n",
      "trie    232 assuming -a 22 array pointer compression\n",
      "trie    112 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:860352 2:19826960 3:96691940 4:221164560 5:346543736\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:860352 2:19826960 3:96691940 4:221164560 5:346543736\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Chain sizes: 1:860352 2:19826960 3:63822368 4:102115784 5:148918864\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:211730796 kB\tVmRSS:28032 kB\tRSSMax:0 kB\tuser:0\tsys:0\tCPU:0\treal:76.0625\n"
     ]
    }
   ],
   "source": [
    "cat train.txt | /home/roberta/kenlm/bin/lmplz -o 5 > amazonLM5.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5n = kenlm.LanguageModel('amazonLM5.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.567223510318378"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ppl = get_ppl(model_5n, train_data)\n",
    "train_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.00883546322021"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ppl = get_ppl(model_5n, valid_data)\n",
    "valid_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6799.379858151767"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i like pandas']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273.66703112404986"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i like this tutu']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557.8821726391418"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', '.']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342.7461203901544"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['who wants dinner?']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.45908777555109"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i want to get a refund']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.24555272015706"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this watch is not what i expected']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.22202767793977"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this fits me perfectly .']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.78485857588134"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this coat fits me perfectly ?']\n",
    "ppl = get_ppl(model_5n, sentences)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-gram model with KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:860352 2:3516892160 3:6594172928 4:10550676480 5:15386402816 6:21101352960 7:27695525888 8:35168919552 9:43521540096 10:52753383424\n",
      "Statistics:\n",
      "1 71696 D1=0.690098 D2=0.962667 D3+=1.22676\n",
      "2 1239185 D1=0.712943 D2=1.05296 D3+=1.36242\n",
      "3 4834597 D1=0.796199 D2=1.09701 D3+=1.35908\n",
      "4 9215190 D1=0.868874 D2=1.16401 D3+=1.3733\n",
      "5 12376562 D1=0.922179 D2=1.23342 D3+=1.42227\n",
      "6 14073204 D1=0.957655 D2=1.31503 D3+=1.47777\n",
      "7 14755602 D1=0.97911 D2=1.4124 D3+=1.53403\n",
      "8 14907447 D1=0.990495 D2=1.48841 D3+=1.62571\n",
      "9 14831157 D1=0.995705 D2=1.56423 D3+=1.6684\n",
      "10 14667984 D1=0.985478 D2=1.6356 D3+=2.00243\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 2227 assuming -p 1.5\n",
      "probing 2720 assuming -r models -p 1.5\n",
      "trie    1154 without quantization\n",
      "trie     631 assuming -q 8 -b 8 quantization \n",
      "trie     981 assuming -a 22 array pointer compression\n",
      "trie     457 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:860352 2:19826960 3:96691940 4:221164560 5:346543736 6:450342528 7:531201672 8:596297880 9:652570908 10:704063232\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:860352 2:19826960 3:96691940 4:221164560 5:346543736 6:450342528 7:531201672 8:596297880 9:652570908 10:704063232\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Chain sizes: 1:860352 2:10898021 3:20433790 4:32694062 5:47678840 6:65388124 7:85821912 8:108980208 9:134863008 10:163470320\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:212009356 kB\tVmRSS:27076 kB\tRSSMax:0 kB\tuser:0\tsys:0\tCPU:0\treal:179.938\n"
     ]
    }
   ],
   "source": [
    "cat train.txt | /home/roberta/kenlm/bin/lmplz -o 10 > amazonLM10.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot read model 'amazonLM10.arpa' (lm/model.cc:49 in void lm::ngram::detail::{anonymous}::CheckCounts(const std::vector<long unsigned int>&) threw FormatLoadException because `counts.size() > 6'. This model has order 10 but KenLM was compiled to support up to 6.  If your build system supports changing KENLM_MAX_ORDER, change it there and recompile.  With cmake:  cmake -DKENLM_MAX_ORDER=10 .. With Moses:  bjam --max-kenlm-order=10 -a Otherwise, edit lm/max_order.hh. Byte: 173)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32mkenlm.pyx\u001b[0m in \u001b[0;36mkenlm.Model.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: lm/model.cc:49 in void lm::ngram::detail::{anonymous}::CheckCounts(const std::vector<long unsigned int>&) threw FormatLoadException because `counts.size() > 6'.\nThis model has order 10 but KenLM was compiled to support up to 6.  If your build system supports changing KENLM_MAX_ORDER, change it there and recompile.  With cmake:\n cmake -DKENLM_MAX_ORDER=10 ..\nWith Moses:\n bjam --max-kenlm-order=10 -a\nOtherwise, edit lm/max_order.hh. Byte: 173",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-f20939908b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_10n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amazonLM10.arpa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mkenlm.pyx\u001b[0m in \u001b[0;36mkenlm.Model.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot read model 'amazonLM10.arpa' (lm/model.cc:49 in void lm::ngram::detail::{anonymous}::CheckCounts(const std::vector<long unsigned int>&) threw FormatLoadException because `counts.size() > 6'. This model has order 10 but KenLM was compiled to support up to 6.  If your build system supports changing KENLM_MAX_ORDER, change it there and recompile.  With cmake:  cmake -DKENLM_MAX_ORDER=10 .. With Moses:  bjam --max-kenlm-order=10 -a Otherwise, edit lm/max_order.hh. Byte: 173)"
     ]
    }
   ],
   "source": [
    "model_10n = kenlm.LanguageModel('amazonLM10.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ppl = get_ppl(model_10n, train_data)\n",
    "train_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ppl = get_ppl(model_10n, valid_data)\n",
    "valid_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons of different ngram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['i like pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl3 = get_ppl(model_3n, sentences)\n",
    "ppl5 = get_ppl(model_5n, sentences)\n",
    "ppl10 = get_ppl(model_10n, sentences)\n",
    "ppl3, ppl5, ppl10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['this shirt fits me very well !']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl3 = get_ppl(model_3n, sentences)\n",
    "ppl5 = get_ppl(model_5n, sentences)\n",
    "ppl10 = get_ppl(model_10n, sentences)\n",
    "ppl3, ppl5, ppl10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['i was very disappointed in the color of these shoes, so I returned them .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl3 = get_ppl(model_3n, sentences)\n",
    "ppl5 = get_ppl(model_5n, sentences)\n",
    "ppl10 = get_ppl(model_10n, sentences)\n",
    "ppl3, ppl5, ppl10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ln_ECRFlNnA"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oov(model, data):\n",
    "    oov = []\n",
    "    vocab = []\n",
    "    for sent in data:\n",
    "        sentence = sent\n",
    "        words =  sentence.split()\n",
    "        vocab += words\n",
    "        # Find out-of-vocabulary words\n",
    "        for w in words:\n",
    "            if w not in model:\n",
    "                    oov.append(w)\n",
    "    return set(oov), set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this is a great tutu and at a really great price . it doesn ' t look cheap at all . i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly . a + + \\n\",\n",
       " 'i bought this for my 4 yr old daughter for dance class , she wore it today for the first time and the teacher thought it was adorable . i bought this to go with a light blue long sleeve leotard and was happy the colors matched up great . price was very good too since some of these go for over $ 15 . 00 dollars . \\n',\n",
       " 'what can i say . . . my daughters have it in orange , black , white and pink and i am thinking to buy for they the fuccia one . it is a very good way for exalt a dancer outfit : great colors , comfortable , looks great , easy to wear , durables and little girls love it . i think it is a great buy for costumer and play too . \\n']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_train = '/home/roberta/ammi-2019-nlp/data/train.txt'\n",
    "train_data = load_data(path_to_train)\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-4244ddbce00a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# oov[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "oov = get_oov(model, data)\n",
    "# oov[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ken_lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
