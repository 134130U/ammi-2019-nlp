{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ngram-lm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/ngram_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "F6K8HrHl-UEb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HhOIi6wOARdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# N-gram Language Modeling"
      ]
    },
    {
      "metadata": {
        "id": "LCVSciOCAMZb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "jYs6AMs6AIre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import torchtext.data as data\n",
        "\n",
        "from torchtext import vocab\n",
        "from collections import Counter\n",
        "import re\n",
        "from torchtext.data import TabularDataset "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H20pktPiA63a",
        "colab_type": "code",
        "outputId": "fb38d897-e889-4451-df77-9ca98eb266a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7eff222cd090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "mUgX68NsBaRf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Language Modeling**\n",
        "\n",
        "**Goal:** compute a probabilty distribution over all possible sentences:\n",
        "\n",
        "\n",
        "$p(W) = p(w_1, w_2, ..., w_T)$ - the probability that the sentence composed of words $(w_1, ..., w_T)$ in this order\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This unsupervised learning problem can be framed as a sequence of supervised learning problems:\n",
        "\n",
        "\n",
        "$p(W) = p(w_1) * p(w_2|w_1) * ... * p(w_T|w_1, ..., w_{T-1})$\n",
        "\n",
        "\n",
        "\n",
        "If we have N sentences, each of them with T words / tokens, then we want to max:\n",
        "\n",
        "$log p(W) = \\sum_{n = 1}^N \\sum_{i=1}^{T} log p(w_i | w_{<i})$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "b1Q51B53E3pn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **N-gram language model**\n",
        "\n",
        "**Goal:** estimate the n-gram probabilities using counts of sequences of n consecutive words\n",
        "\n",
        "Given a sequence of words $w$, we want to compute\n",
        "\n",
        "  $P(w_i|w_{i−1}, w_{i−2}, …, w_{i−n+1})$\n",
        "\n",
        "Where $w_i$ is the i-th word of the sequence.\n",
        "\n",
        "$P(w_i|w_{i−n+1}, ..., w_{i−2}, w_{i−1}) = \\frac{p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w_i)}{\\sum_{w \\in V} p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w)}$\n",
        "\n",
        "**Key Idea:** We can estimate the probabilities using counts of n-grams in our dataset \n"
      ]
    },
    {
      "metadata": {
        "id": "pkb6-i6cJRox",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ]
    },
    {
      "metadata": {
        "id": "k0zpbq6AJRy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bag of N-grams"
      ]
    },
    {
      "metadata": {
        "id": "dToAUzUkJR65",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train, Validation, Test Datasets"
      ]
    },
    {
      "metadata": {
        "id": "2ATBQ4WaJR93",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "UTvSo3D3JSBD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loss Function and Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "nxfi7ruGHZu3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "metadata": {
        "id": "lhi0Ww8JKEkk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis & Examples"
      ]
    },
    {
      "metadata": {
        "id": "6jzyCRhtBNit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFDnznsFJRMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}