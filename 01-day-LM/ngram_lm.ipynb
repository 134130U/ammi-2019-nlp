{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/ngram_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhOIi6wOARdm"
   },
   "source": [
    "# N-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: should we install as needed and import as needed or all at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run if you dont have it installed\n",
    "# !pip install more_itertools\n",
    "# !pip install spacy# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\\\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCVSciOCAMZb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYs6AMs6AIre"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy\n",
    "import itertools\n",
    "from operator import itemgetter \n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import re\n",
    "import more_itertools as mit  # not built-in package\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "from torchtext import vocab\n",
    "from collections import Counter\n",
    "import re\n",
    "from torchtext.data import TabularDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H20pktPiA63a",
    "outputId": "fb38d897-e889-4451-df77-9ca98eb266a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd43a2de2b0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsDataset(TabularDataset):\n",
    "    \n",
    "    urls = [\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz',\n",
    "           'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Tools_and_Home_Improvement_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Patio_Lawn_and_Garden_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Baby_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz',\n",
    "#             'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz',\n",
    "        ]\n",
    "    name='amazonreviews'\n",
    "    dirname='processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_done = AmazonReviewsDataset.download(root='data/', check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)\n",
    "\n",
    "# def tokenize(s):\n",
    "#     return RETOK.findall(s)\n",
    "\n",
    "# text_field = data.Field(sequential=True, tokenize=tokenize, include_lengths=True, use_vocab=True, lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = AmazonReviewsDataset(path='/home/roberta/ammi-2019-nlp/01-day-LM/data/amazonreviews/reviews_Sports_and_Outdoors_5.json', format='json', fields={'reviewText': ('reviewText', text_field), 'summary': ('summary', text_field)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets check it\n",
    "# # lets use fstrings btw\n",
    "# print(f'Number of samples : {len(dataset.examples)}')\n",
    "\n",
    "# for ex in dataset.examples:\n",
    "#     print(f'Review: \\n {ex.reviewText} \\n\\n Summary: \\n {ex.summary}')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the dataset to a list of strings\n",
    "# # each string represents a review\n",
    "# all_reviews = []\n",
    "# for ex in dataset.examples:\n",
    "#     all_reviews.append(ex.reviewText)\n",
    "# len(all_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tqdm = tqdm_notebook  # prolly you need jupyter widget for this, change for tqdm for simple tqdm\n",
    "\n",
    "# NUM_SENTENCES = len(all_reviews)\n",
    "# NUM_SENTENCES_TRAIN = int(0.8*NUM_SENTENCES)\n",
    "# NUM_SENTENCES_TEST = int(0.1*NUM_SENTENCES)\n",
    "# NUM_SENTENCES_VALID = NUM_SENTENCES - NUM_SENTENCES_TRAIN - NUM_SENTENCES_TEST\n",
    "\n",
    "# train_reviews = all_reviews[:NUM_SENTENCES_TRAIN]\n",
    "# test_reviews = all_reviews[NUM_SENTENCES_TRAIN:NUM_SENTENCES_TRAIN+NUM_SENTENCES_TEST]\n",
    "# valid_reviews = all_reviews[NUM_SENTENCES_TRAIN+NUM_SENTENCES_TEST:NUM_SENTENCES_TRAIN+NUM_SENTENCES_TEST+NUM_SENTENCES_VALID]\n",
    "\n",
    "# train_reviews = [str(t) for t in train_reviews]\n",
    "# test_reviews = [str(t) for t in test_reviews]\n",
    "# valid_reviews = [str(t) for t in valid_reviews]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO make a function to convert list of urls to txt files with train, test, and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/amazon_reviews_sports_train.txt', 'w') as f:\n",
    "#     for item in train_reviews:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/amazon_reviews_sports_valid.txt', 'w') as f:\n",
    "#     for item in valid_reviews:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/amazon_reviews_sports_test.txt', 'w') as f:\n",
    "#     for item in test_reviews:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data From .txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Data\n",
    "train_data = []\n",
    "with open('../data/amazon_reviews_sports_train.txt', 'r') as f:\n",
    "    for item in train_reviews:\n",
    "        train_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237069,\n",
       " list,\n",
       " \"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\",\n",
       " str,\n",
       " 226)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Train Data        \n",
    "len(train_data), type(train_data), train_data[0], type(train_data[0]), len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data\n",
    "valid_data = []\n",
    "with open('../data/amazon_reviews_sports_valid.txt', 'r') as f:\n",
    "    for item in valid_reviews:\n",
    "        valid_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29635,\n",
       " list,\n",
       " \"['great', 'sling', 'bag', '.', 'i', 'used', 'it', 'for', 'a', 'trip', 'to', 'disney', 'and', 'this', 'bag', 'worked', 'very', 'well', '.', 'it', 'has', 'enough', 'pockets', 'to', 'carry', 'all', 'the', 'essentials', 'needed', 'for', 'a', 'vacation', 'to', 'disney', 'in', 'june', '.', 'i', 'was', 'able', 'to', 'carry', 'so', 'mush', 'stuff', 'in', 'this', 'bag', 'my', 'wife', 'did', 'not', 'have', 'to', 'carry', 'a', 'back', 'pack', '.', 'i', 'gonna', 'to', 'carry', 'this', 'bag', 'for', 'all', 'my', 'trips', 'from', 'now', 'on', 'and', 'it', 'worked', 'well', 'through', 'all', 'the', 'airport', 'security', 'checks', '.', 'i', 'highly', 'recommend', 'this', 'bag', '.']\",\n",
       " str,\n",
       " 676)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Valid Data\n",
    "len(valid_data), type(valid_data), valid_data[0], type(valid_data[0]), len(valid_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "test_data = []\n",
    "with open('../data/amazon_reviews_sports_test.txt', 'r') as f:\n",
    "    for item in test_reviews:\n",
    "        test_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29633,\n",
       " list,\n",
       " '[\\'this\\', \\'is\\', \\'the\\', \\'most\\', \\'affordable\\', \\'/\\', \\'highest\\', \\'quality\\', \\'made\\', \\'in\\', \\'the\\', \\'usa\\', \\'knife\\', \\'i\\', \\'have\\', \\'found\\', \\'.\\', \\'the\\', \\'knife\\', \\'is\\', \\'very\\', \\'lightweight\\', \\',\\', \\'lighter\\', \\'than\\', \\'my\\', \\'kershaw\\', \\'blur\\', \\'and\\', \\'its\\', \\'a\\', \\'bit\\', \\'thinner\\', \\'too\\', \\'.\\', \\'typically\\', \\'really\\', \\'lightweight\\', \\'knives\\', \\'have\\', \\'frn\\', \\'handles\\', \\'like\\', \\'the\\', \\'endura\\', \\'4\\', \\'or\\', \\'the\\', \\'griptillian\\', \\'and\\', \\'that\\', \"\\'\", \\'s\\', \\'fine\\', \\'but\\', \\'that\\', \\'stuff\\', \\'does\\', \\'flex\\', \\'if\\', \\'u\\', \\'squeeze\\', \\'hard\\', \\'enough\\', \\'.\\', \\'the\\', \\'aluminum\\', \\'handles\\', \\'in\\', \\'the\\', \\'knockout\\', \\'feel\\', \\'really\\', \\'strong\\', \\'and\\', \\'have\\', \\'no\\', \\'give\\', \\'to\\', \\'them\\', \\'and\\', \\'i\\', \\'prefer\\', \\'the\\', \\'feel\\', \\'of\\', \\'metal\\', \\'over\\', \\'plastic\\', \\'any\\', \\'day\\', \\'.\\', \\'there\\', \\'was\\', \\'a\\', \\'little\\', \\'rattle\\', \\'if\\', \\'i\\', \\'shook\\', \\'the\\', \\'blade\\', \\'but\\', \\'that\\', \\'was\\', \\'fixed\\', \\'with\\', \\'a\\', \\'little\\', \\'petroleum\\', \\'jelly\\', \\'on\\', \\'the\\', \\'torsion\\', \\'bar\\', \\'.\\', \\'the\\', \\'knockout\\', \\'could\\', \\'use\\', \\'some\\', \\'jimping\\', \\'on\\', \\'the\\', \\'spine\\', \\'and\\', \\'in\\', \\'places\\', \\'on\\', \\'the\\', \\'handle\\', \\'to\\', \\'make\\', \\'it\\', \\'less\\', \\'slick\\', \\'and\\', \\'that\\', \"\\'\", \\'s\\', \\'the\\', \\'only\\', \\'knock\\', \\'i\\', \\'can\\', \\'give\\', \\'the\\', \\'knockoutupdate\\', \\':\\', \\'the\\', \\'more\\', \\'i\\', \\'used\\', \\'this\\', \\'knife\\', \\',\\', \\'the\\', \\'more\\', \\'i\\', \\'loved\\', \\'it\\', \\'.\\', \\'after\\', \\'a\\', \\'while\\', \\'you\\', \\'get\\', \\'used\\', \\'to\\', \\'no\\', \\'jimping\\', \\'and\\', \\'its\\', \\'fine\\', \\',\\', \\'the\\', \\'knife\\', \\'has\\', \\'remained\\', \\'rock\\', \\'solid\\', \\'with\\', \\'no\\', \\'bladeplay\\', \\',\\', \\'and\\', \\'it\\', \\'keeps\\', \\'an\\', \\'excellent\\', \\'edge\\', \\'.\\', \\'if\\', \\'i\\', \\'ever\\', \\'lost\\', \\'this\\', \\'i\\', \"\\'\", \\'d\\', \\'be\\', \\'furious\\', \\'but\\', \\'i\\', \\'would\\', \\'absolutely\\', \\'buy\\', \\'another\\', \\'one\\', \\'.\\']',\n",
       " str,\n",
       " 1645)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Test Data\n",
    "len(test_data), type(test_data), test_data[0], type(test_data[0]), len(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')               \n",
    "punctuations = string.punctuation   \n",
    "TAG_RE = re.compile(r'<[^>]+>') # get rid off HTML tags from the data\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)] #and (token.is_stop is False)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "   # tokenize each sentence -- each tokenized sentence will be an element in token_dataset\n",
    "    token_dataset = []\n",
    "    # tokenize all words -- each token will be an item in all_tokens (in the order given by the list of sentences)\n",
    "    all_tokens = []     # all the tokens -- \n",
    "\n",
    "    for sample in tqdm(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'<[^>]+>', re.UNICODE)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\",\n",
       " '[\\'i\\', \\'had\\', \\'a\\', \\'factory\\', \\'glock\\', \\'tool\\', \\'that\\', \\'i\\', \\'was\\', \\'using\\', \\'for\\', \\'my\\', \\'glock\\', \\'26\\', \\',\\', \\'27\\', \\',\\', \\'and\\', \\'17\\', \\'.\\', \\'i\\', \"\\'\", \\'ve\\', \\'since\\', \\'lost\\', \\'it\\', \\'and\\', \\'had\\', \\'needed\\', \\'another\\', \\'.\\', \\'since\\', \\'i\\', \"\\'\", \\'ve\\', \\'used\\', \\'ghost\\', \\'products\\', \\'prior\\', \\',\\', \\'and\\', \\'know\\', \\'that\\', \\'they\\', \\'are\\', \\'reliable\\', \\',\\', \\'i\\', \\'had\\', \\'decided\\', \\'to\\', \\'order\\', \\'this\\', \\'one\\', \\'.\\', \\'sure\\', \\'enough\\', \\',\\', \\'this\\', \\'is\\', \\'just\\', \\'as\\', \\'good\\', \\'as\\', \\'a\\', \\'factory\\', \\'tool\\', \\'.\\']']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for now only work with small subset of the data -- switch to all data later\n",
    "train_data = train_data[:800]\n",
    "test_data = test_data[:100]\n",
    "valid_data = valid_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\",\n",
       " 800)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "21it [00:00, 208.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "45it [00:00, 215.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "60it [00:00, 189.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "73it [00:00, 161.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "90it [00:00, 161.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "105it [00:00, 155.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "124it [00:00, 164.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "147it [00:00, 179.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "165it [00:00, 170.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "182it [00:01, 134.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "197it [00:01, 130.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "218it [00:01, 144.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "234it [00:01, 147.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "255it [00:01, 160.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "278it [00:01, 176.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "312it [00:01, 204.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "337it [00:01, 215.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "366it [00:01, 232.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "391it [00:02, 227.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "415it [00:02, 213.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "438it [00:02, 193.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "459it [00:02, 177.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "478it [00:02, 175.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "501it [00:02, 186.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "521it [00:02, 176.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "540it [00:02, 180.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "562it [00:03, 181.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "581it [00:03, 145.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "601it [00:03, 150.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "630it [00:03, 175.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "650it [00:03, 144.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "673it [00:03, 160.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "693it [00:03, 170.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "712it [00:04, 166.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "730it [00:04, 169.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "748it [00:04, 143.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "764it [00:04, 124.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "778it [00:04, 125.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "800it [00:04, 172.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "14it [00:00, 135.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "35it [00:00, 150.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "59it [00:00, 169.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "86it [00:00, 190.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100it [00:00, 218.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "14it [00:00, 137.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "42it [00:00, 161.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "70it [00:00, 184.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "95it [00:00, 199.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100it [00:00, 233.56it/s]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = tokenize_dataset(train_data)\n",
    "test_data_tokenized, all_tokens_test = tokenize_dataset(test_data)\n",
    "valid_data_tokenized, all_tokens_valid = tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tokenized data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,\n",
       " [['this',\n",
       "   'came',\n",
       "   'in',\n",
       "   'on',\n",
       "   'time',\n",
       "   'and',\n",
       "   'i',\n",
       "   'am',\n",
       "   'veru',\n",
       "   'happy',\n",
       "   'with',\n",
       "   'it',\n",
       "   'i',\n",
       "   'haved',\n",
       "   'used',\n",
       "   'it',\n",
       "   'already',\n",
       "   'and',\n",
       "   'it',\n",
       "   'makes',\n",
       "   'taking',\n",
       "   'out',\n",
       "   'the',\n",
       "   'pins',\n",
       "   'in',\n",
       "   'my',\n",
       "   'glock',\n",
       "   '32',\n",
       "   'very',\n",
       "   'easy'],\n",
       "  ['i',\n",
       "   'had',\n",
       "   'a',\n",
       "   'factory',\n",
       "   'glock',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'i',\n",
       "   'was',\n",
       "   'using',\n",
       "   'for',\n",
       "   'my',\n",
       "   'glock',\n",
       "   '26',\n",
       "   '27',\n",
       "   'and',\n",
       "   '17',\n",
       "   'i',\n",
       "   've',\n",
       "   'since',\n",
       "   'lost',\n",
       "   'it',\n",
       "   'and',\n",
       "   'had',\n",
       "   'needed',\n",
       "   'another',\n",
       "   'since',\n",
       "   'i',\n",
       "   've',\n",
       "   'used',\n",
       "   'ghost',\n",
       "   'products',\n",
       "   'prior',\n",
       "   'and',\n",
       "   'know',\n",
       "   'that',\n",
       "   'they',\n",
       "   'are',\n",
       "   'reliable',\n",
       "   'i',\n",
       "   'had',\n",
       "   'decided',\n",
       "   'to',\n",
       "   'order',\n",
       "   'this',\n",
       "   'one',\n",
       "   'sure',\n",
       "   'enough',\n",
       "   'this',\n",
       "   'is',\n",
       "   'just',\n",
       "   'as',\n",
       "   'good',\n",
       "   'as',\n",
       "   'a',\n",
       "   'factory',\n",
       "   'tool']],\n",
       " [\"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\",\n",
       "  '[\\'i\\', \\'had\\', \\'a\\', \\'factory\\', \\'glock\\', \\'tool\\', \\'that\\', \\'i\\', \\'was\\', \\'using\\', \\'for\\', \\'my\\', \\'glock\\', \\'26\\', \\',\\', \\'27\\', \\',\\', \\'and\\', \\'17\\', \\'.\\', \\'i\\', \"\\'\", \\'ve\\', \\'since\\', \\'lost\\', \\'it\\', \\'and\\', \\'had\\', \\'needed\\', \\'another\\', \\'.\\', \\'since\\', \\'i\\', \"\\'\", \\'ve\\', \\'used\\', \\'ghost\\', \\'products\\', \\'prior\\', \\',\\', \\'and\\', \\'know\\', \\'that\\', \\'they\\', \\'are\\', \\'reliable\\', \\',\\', \\'i\\', \\'had\\', \\'decided\\', \\'to\\', \\'order\\', \\'this\\', \\'one\\', \\'.\\', \\'sure\\', \\'enough\\', \\',\\', \\'this\\', \\'is\\', \\'just\\', \\'as\\', \\'good\\', \\'as\\', \\'a\\', \\'factory\\', \\'tool\\', \\'.\\']'])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Tokenized Sentences == Number of All Sentences in the Dataset\n",
    "# Compare the Tokenized Sentences with the Original Ones\n",
    "len(train_data_tokenized), train_data_tokenized[:2], train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77653, ['this', 'came'])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of All Tokens\n",
    "len(all_tokens_train), all_tokens_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Vocabulary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocabulary size: 6280 words\n"
     ]
    }
   ],
   "source": [
    "# TODO: do we use both train and valid and not test for this??\n",
    "voc = list(set(all_tokens_train + all_tokens_valid))\n",
    "print('Word vocabulary size: {} words'.format(len(voc)))             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS ANALYSIS (Train + Valid Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Tokens in the Corpus Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All Tokens  85062\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All Tokens \", len(all_tokens_train) + len(all_tokens_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All UNIQUE Tokens  6280\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All UNIQUE Tokens \", len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Sentences in the Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences  800 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Sentences \", len(train_data_tokenized), len(valid_data_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count how often each sentence length occurs. Visualize this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of words and their corresponding frequencies. Which are the 10 most frequent words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad sentences with special markers sentence beginning and end, i.e. $<bos>$ and $<eos>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        padded = [\"<bos>\" for i in range((n - 1))] + l +[\"<eos>\" for i in range((n - 1))]\n",
    "        result_list.append(padded)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM = 2  # change this to make different N grams for each token\n",
    "\n",
    "train_data_padded = pad_sentences(train_data_tokenized, NGRAM)\n",
    "test_data_padded = pad_sentences(test_data_tokenized, NGRAM)\n",
    "valid_data_padded = pad_sentences(valid_data_tokenized, NGRAM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our padding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " 'this',\n",
       " 'came',\n",
       " 'in',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'veru',\n",
       " 'happy',\n",
       " 'with',\n",
       " 'it',\n",
       " 'i',\n",
       " 'haved',\n",
       " 'used',\n",
       " 'it',\n",
       " 'already',\n",
       " 'and',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'taking',\n",
       " 'out',\n",
       " 'the',\n",
       " 'pins',\n",
       " 'in',\n",
       " 'my',\n",
       " 'glock',\n",
       " '32',\n",
       " 'very',\n",
       " 'easy',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for finding all N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        result_list.append(list(zip(*[l[i:] for i in range(n)])))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the dataset to its corresponding n-gram version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM = 2  # change this to make different N grams for each token\n",
    "\n",
    "# now make train and valid dicts\n",
    "train_data_ngram = find_ngrams(train_data_padded, NGRAM)\n",
    "valid_data_ngram = find_ngrams(valid_data_padded, NGRAM)\n",
    "test_data_ngram = find_ngrams(test_data_padded, NGRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our n-grams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<bos>', 'this'),\n",
       " ('this', 'came'),\n",
       " ('came', 'in'),\n",
       " ('in', 'on'),\n",
       " ('on', 'time'),\n",
       " ('time', 'and'),\n",
       " ('and', 'i'),\n",
       " ('i', 'am'),\n",
       " ('am', 'veru'),\n",
       " ('veru', 'happy'),\n",
       " ('happy', 'with'),\n",
       " ('with', 'it'),\n",
       " ('it', 'i'),\n",
       " ('i', 'haved'),\n",
       " ('haved', 'used'),\n",
       " ('used', 'it'),\n",
       " ('it', 'already'),\n",
       " ('already', 'and'),\n",
       " ('and', 'it'),\n",
       " ('it', 'makes'),\n",
       " ('makes', 'taking'),\n",
       " ('taking', 'out'),\n",
       " ('out', 'the'),\n",
       " ('the', 'pins'),\n",
       " ('pins', 'in'),\n",
       " ('in', 'my'),\n",
       " ('my', 'glock'),\n",
       " ('glock', '32'),\n",
       " ('32', 'very'),\n",
       " ('very', 'easy'),\n",
       " ('easy', '<eos>')]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_ngram[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a complete list of trigrams occurring in the corpus. Which are the 10 most frequent trigrams?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine count statistics on the trigram frequencies, i.e. compute so-called count-counts (how many trigrams occur once, twice, : : :). Plot their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary of N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 100000\n",
    "\n",
    "# save index 1 for unk, 0 for pad, 1 for bos, 2 for eos\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "all_train_tokens = list(mit.flatten(train_data_ngram + valid_data_ngram))\n",
    "counted_tokens = Counter(all_train_tokens)\n",
    "\n",
    "vocab, count = zip(*counted_tokens.most_common(max_vocab_size))\n",
    "id2token = list(vocab)\n",
    "token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "id2token = ['<pad>', '<unk>', '<bos>', '<eos>'] + id2token\n",
    "\n",
    "token2id['<pad>'] = PAD_IDX \n",
    "token2id['<unk>'] = UNK_IDX\n",
    "token2id['<bos>'] = BOS_IDX \n",
    "token2id['<eos>'] = EOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85962, 41951, 41951)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at some numbers!\n",
    "len(all_train_tokens), len(vocab), len(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the dictionary by loading random token from it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 24749 ; token ('belt', 'just')\n",
      "Token ('belt', 'just'); token id 24747\n"
     ]
    }
   ],
   "source": [
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of trigrams in the corpus using only the words in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate count statistics of the trigram frequencies for this modified corpus as well. What do you notice in comparison to the previous exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the out-of-vocabulary (OOV) rate, i.e. the percentage of running words in the corpus which are not covered by the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Converting from Token to ID and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _text2id(doc):\n",
    "    return [token2id[t] if t in token2id else UNK_IDX for t in doc]\n",
    "\n",
    "def _id2text(vec):\n",
    "    return [id2token[i] for i in vec]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_id = []\n",
    "for d in train_data_ngram:\n",
    "    train_data_id.append(_text2id(d))\n",
    "    \n",
    "valid_data_id = []\n",
    "for d in valid_data_ngram:\n",
    "    valid_data_id.append(_text2id(d))\n",
    "    \n",
    "train_data_id_merged = []\n",
    "for d in train_data_id:\n",
    "    train_data_id_merged.append((d, 0))\n",
    "\n",
    "valid_data_id_merged = []\n",
    "for d in valid_data_id:\n",
    "    valid_data_id_merged.append((d, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([21, 9863],\n",
       " [('<bos>', 'this'), ('this', 'came')],\n",
       " ([21,\n",
       "   9863,\n",
       "   2505,\n",
       "   3415,\n",
       "   3416,\n",
       "   367,\n",
       "   16,\n",
       "   49,\n",
       "   9864,\n",
       "   9865,\n",
       "   396,\n",
       "   133,\n",
       "   136,\n",
       "   9866,\n",
       "   9867,\n",
       "   432,\n",
       "   5237,\n",
       "   9868,\n",
       "   18,\n",
       "   884,\n",
       "   9869,\n",
       "   9870,\n",
       "   321,\n",
       "   9871,\n",
       "   9872,\n",
       "   28,\n",
       "   1313,\n",
       "   9873,\n",
       "   9874,\n",
       "   699,\n",
       "   9875],\n",
       "  0),\n",
       " 800,\n",
       " 800)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at some numbers\n",
    "train_data_id[0][:2], train_data_ngram[0][:2], train_data_id_merged[0], len(train_data_id_merged), len(train_data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making PyTorch Dataset out of our set of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, data_list, max_inp_length=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        data_list is a list of tuples: (x,y) where x is a list of ids and y is a label\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.max_len = max_inp_length\n",
    "        self.data_tensors = []\n",
    "        for (i, t) in tqdm_notebook(self.data):\n",
    "            self.data_tensors.append((torch.LongTensor(i[:self.max_len]), torch.LongTensor([t])))\n",
    "            #TODO: fix error regarding to(device)\n",
    "#             self.data_tensors.append((torch.LongTensor(i[:self.max_len]).to(device), torch.LongTensor([t]).to(device)))\n",
    "              \n",
    "    def __getitem__(self, key):\n",
    "        (inp, tgt) = self.data_tensors[key]\n",
    "        \n",
    "        return inp, tgt, len(inp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def pad(tensor, length, dim=0, pad=0):\n",
    "    \"\"\"Pad tensor to a specific length.\n",
    "    :param tensor: vector to pad\n",
    "    :param length: new length\n",
    "    :param dim: (default 0) dimension to pad\n",
    "    :returns: padded tensor if the tensor is shorter than length\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) < length:\n",
    "        return torch.cat(\n",
    "            [tensor, tensor.new(*tensor.size()[:dim],\n",
    "                                length - tensor.size(dim),\n",
    "                                *tensor.size()[dim + 1:]).fill_(pad)],\n",
    "            dim=dim)\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def batchify(batch):\n",
    "    maxlen = max(batch, key = itemgetter(2))[-1]\n",
    "    batch_list = []\n",
    "    target_list = []\n",
    "    for b in batch:\n",
    "        batch_list.append(pad(b[0], maxlen, dim=0, pad=PAD_IDX))\n",
    "        target_list.append(b[1])\n",
    "    input_batch = torch.stack(batch_list, 0)\n",
    "    target_batch = torch.stack(target_list, 0)\n",
    "    \n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9f0686ac404b8ca065447e6db9e8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9650c6cbb6aa409f83f004d4221e3081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ImdbDataset(train_data_id_merged, max_inp_length=None, device='cuda')\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, collate_fn=batchify, shuffle=True)\n",
    "\n",
    "valid_dataset = ImdbDataset(valid_data_id_merged, max_inp_length=None, device='cuda')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec:  19%|█▉        | 1.25G/6.60G [11:49<50:35, 1.76MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url)\u001b[0m\n\u001b[1;32m    260\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3abfa44c94ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.vector_cache'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url)\u001b[0m\n\u001b[1;32m    261\u001b[0m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting vectors into {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = vocab.FastText(language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(dataset, max_size=30000, vectors=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a batch iterator\n",
    "train_loader = data.BucketIterator(dataset=dataset, batch_size=4, sort_key=lambda x: len(x.reviewText), device=torch.device('cpu'), sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vec2txt(vec):\n",
    "    return [text_field.vocab.itos[t] for t in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.reviewText[0][0])\n",
    "print(_vec2txt(batch.reviewText[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUgX68NsBaRf"
   },
   "source": [
    "# **Language Modeling**\n",
    "\n",
    "**Goal:** compute a probabilty distribution over all possible sentences:\n",
    "\n",
    "\n",
    "$p(W) = p(w_1, w_2, ..., w_T)$ - the probability that the sentence composed of words $(w_1, ..., w_T)$ in this order\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This unsupervised learning problem can be framed as a sequence of supervised learning problems:\n",
    "\n",
    "\n",
    "$p(W) = p(w_1) * p(w_2|w_1) * ... * p(w_T|w_1, ..., w_{T-1})$\n",
    "\n",
    "\n",
    "\n",
    "If we have N sentences, each of them with T words / tokens, then we want to max:\n",
    "\n",
    "$log p(W) = \\sum_{n = 1}^N \\sum_{i=1}^{T} log p(w_i | w_{<i})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1Q51B53E3pn"
   },
   "source": [
    "# **N-gram language model**\n",
    "\n",
    "**Goal:** estimate the n-gram probabilities using counts of sequences of n consecutive words\n",
    "\n",
    "Given a sequence of words $w$, we want to compute\n",
    "\n",
    "  $P(w_i|w_{i−1}, w_{i−2}, …, w_{i−n+1})$\n",
    "\n",
    "Where $w_i$ is the i-th word of the sequence.\n",
    "\n",
    "$P(w_i|w_{i−n+1}, ..., w_{i−2}, w_{i−1}) = \\frac{p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w_i)}{\\sum_{w \\in V} p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w)}$\n",
    "\n",
    "**Key Idea:** We can estimate the probabilities using counts of n-grams in our dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkb6-i6cJRox"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0zpbq6AJRy2"
   },
   "source": [
    "# Bag of N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dToAUzUkJR65"
   },
   "source": [
    "# Train, Validation, Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ATBQ4WaJR93"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTvSo3D3JSBD"
   },
   "source": [
    "# Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxfi7ruGHZu3"
   },
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhi0Ww8JKEkk"
   },
   "source": [
    "# Analysis & Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jzyCRhtBNit"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFDnznsFJRMI"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "ngram-lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
