{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('utils/')\n",
    "import loading_text_and_tokenization\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.ngram_utils as ngram_utils\n",
    "from utils.ngram_utils import NgramLM\n",
    "from utils.amazon_dataset import AmazonDataset, pad, batchify\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.neural_lm import BagOfNGrams, DecoderMLP, seq2seq\n",
    "import utils.global_variables as gl\n",
    "import torch\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "_tqdm = tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f588c114790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .txt files and create lists of reviews\n",
    "train_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/smallish_train.txt', 'r') as f:\n",
    "    train_data = [review for review in f.read().split('\\n') if review]\n",
    "    \n",
    "valid_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/smallish_valid.txt', 'r') as f:\n",
    "    valid_data = [review for review in f.read().split('\\n') if review]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"this is a great tutu and at a really great price . it doesn ' t look cheap at all . i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly . a + + \",\n",
       " list,\n",
       " 11144,\n",
       " str)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], valid_data[0]\n",
    "train_data = train_data#[:100]\n",
    "valid_data = valid_data#[:10]\n",
    "train_data[0], type(train_data), len(train_data), type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434a549985a441af976c75202a6a8f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2e5ed45c4d4f728523ea21260716c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = ngram_utils.tokenize_dataset(train_data)\n",
    "valid_data_tokenized, all_tokens_valid = ngram_utils.tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_padded = ngram_utils.pad_dataset(train_data_tokenized, n=N)\n",
    "valid_data_padded = ngram_utils.pad_dataset(valid_data_tokenized, n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14979, ('<sos>', '<eos>', '.', 'the', 'i', ',', 'and', 'a', 'to', \"'\"))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ngram_utils.get_vocab(train_data_padded)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14983, 14981)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token, token2id = ngram_utils.get_dict(vocab)\n",
    "len(id2token), len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ids = ngram_utils.get_ids(train_data_padded, token2id)\n",
    "valid_data_ids = ngram_utils.get_ids(valid_data_padded, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53681/53681 [00:04<00:00, 12015.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(train_data_ids, max_inp_length=None, use_cuda=True)\n",
    "train_dataset_ngrams = []\n",
    "for t in train_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        train_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "train_loader = DataLoader(train_dataset_ngrams, batch_size=2048, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6067/6067 [00:00<00:00, 31304.05it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = AmazonDataset(valid_data_ids, max_inp_length=None, use_cuda=True)\n",
    "valid_dataset_ngrams = []\n",
    "for t in valid_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        valid_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "valid_loader = DataLoader(valid_dataset_ngrams, batch_size=2048, collate_fn=batchify, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868406, 95092)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(train_dataset_ngrams)\n",
    "num_valid = len(valid_dataset_ngrams)\n",
    "num_train, num_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfNGrams(\n",
       "  (embedding): EmbeddingBag(14983, 300, mode=mean)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = BagOfNGrams(len(id2token), emb_dim=300, hidden_size=256, out_size=128, activation='ReLU', nlayers=2, reduce='mean', dropout=0.1, batch_norm=False)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderMLP(\n",
       "  (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=14983, bias=True)\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderMLP(input_size=128, output_size=len(id2token), hidden_size=256)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): BagOfNGrams(\n",
       "    (embedding): EmbeddingBag(14983, 300, mode=mean)\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderMLP(\n",
       "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=14983, bias=True)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq2seq(encoder, decoder, id2token, use_cuda=False, lr=0.1, size_ngrams=N) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 | Train Loss  9.85 | Train PPL 18969.54 |    10/  424 Batches\n",
      "| Epoch   0 | Train Loss  7.96 | Train PPL  2867.69 |    20/  424 Batches\n",
      "| Epoch   0 | Train Loss  7.01 | Train PPL  1109.46 |    30/  424 Batches\n",
      "| Epoch   0 | Train Loss  6.48 | Train PPL   655.03 |    40/  424 Batches\n",
      "| Epoch   0 | Train Loss  6.12 | Train PPL   455.33 |    50/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.85 | Train PPL   347.15 |    60/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.71 | Train PPL   300.87 |    70/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.62 | Train PPL   274.93 |    80/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.58 | Train PPL   264.26 |    90/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.40 | Train PPL   222.35 |   100/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.44 | Train PPL   231.32 |   110/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.36 | Train PPL   213.79 |   120/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.30 | Train PPL   201.16 |   130/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.27 | Train PPL   194.55 |   140/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.23 | Train PPL   187.18 |   150/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.20 | Train PPL   181.23 |   160/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.10 | Train PPL   163.35 |   170/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.07 | Train PPL   159.84 |   180/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.07 | Train PPL   159.59 |   190/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.01 | Train PPL   149.33 |   200/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.98 | Train PPL   145.69 |   210/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.99 | Train PPL   146.26 |   220/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.94 | Train PPL   140.29 |   230/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.91 | Train PPL   135.95 |   240/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.96 | Train PPL   142.90 |   250/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.86 | Train PPL   129.17 |   260/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.90 | Train PPL   134.82 |   270/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.88 | Train PPL   131.33 |   280/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.89 | Train PPL   132.77 |   290/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.88 | Train PPL   131.42 |   300/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.85 | Train PPL   127.51 |   310/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.81 | Train PPL   122.82 |   320/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.79 | Train PPL   120.46 |   330/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.80 | Train PPL   121.74 |   340/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.77 | Train PPL   117.83 |   350/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.73 | Train PPL   113.71 |   360/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.71 | Train PPL   111.08 |   370/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.71 | Train PPL   111.00 |   380/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.72 | Train PPL   111.79 |   390/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.70 | Train PPL   110.26 |   400/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.68 | Train PPL   107.48 |   410/  424 Batches\n",
      "| Epoch   0 | Train Loss  4.68 | Train PPL   107.82 |   420/  424 Batches\n",
      "| Epoch   0 | Train Loss  5.31 | Train PPL   202.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   0 | Valid Loss  4.49 | Valid PPL    88.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Train Loss  5.10 | Train PPL   163.85 |    10/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.60 | Train PPL    99.28 |    20/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.65 | Train PPL   104.39 |    30/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.63 | Train PPL   103.00 |    40/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.59 | Train PPL    98.78 |    50/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.64 | Train PPL   103.19 |    60/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.64 | Train PPL   103.71 |    70/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.60 | Train PPL    99.36 |    80/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.64 | Train PPL   103.32 |    90/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.61 | Train PPL   100.01 |   100/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.55 | Train PPL    94.66 |   110/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.57 | Train PPL    96.92 |   120/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.54 | Train PPL    93.83 |   130/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.61 | Train PPL   100.23 |   140/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.61 | Train PPL   100.98 |   150/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.56 | Train PPL    95.56 |   160/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.53 | Train PPL    93.05 |   170/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.56 | Train PPL    95.13 |   180/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.55 | Train PPL    94.46 |   190/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.54 | Train PPL    93.38 |   200/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.53 | Train PPL    92.72 |   210/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.54 | Train PPL    93.79 |   220/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.52 | Train PPL    91.68 |   230/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.53 | Train PPL    92.79 |   240/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.51 | Train PPL    90.71 |   250/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.50 | Train PPL    89.66 |   260/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.54 | Train PPL    93.71 |   270/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.52 | Train PPL    91.69 |   280/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.50 | Train PPL    90.28 |   290/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.56 | Train PPL    95.60 |   300/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.49 | Train PPL    88.98 |   310/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.47 | Train PPL    87.26 |   320/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.50 | Train PPL    89.73 |   330/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.51 | Train PPL    90.74 |   340/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.50 | Train PPL    90.26 |   350/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.52 | Train PPL    92.19 |   360/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.46 | Train PPL    86.52 |   370/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.46 | Train PPL    86.67 |   380/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.51 | Train PPL    90.64 |   390/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.44 | Train PPL    85.16 |   400/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.51 | Train PPL    90.51 |   410/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.49 | Train PPL    89.19 |   420/  424 Batches\n",
      "| Epoch   1 | Train Loss  4.55 | Train PPL    94.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Valid Loss  4.27 | Valid PPL    71.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Train Loss  4.86 | Train PPL   128.83 |    10/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.39 | Train PPL    80.37 |    20/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.43 | Train PPL    84.17 |    30/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.44 | Train PPL    84.41 |    40/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.45 | Train PPL    85.33 |    50/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.41 | Train PPL    82.07 |    60/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.41 | Train PPL    82.10 |    70/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.43 | Train PPL    83.61 |    80/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.77 |    90/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.43 | Train PPL    83.77 |   100/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.43 | Train PPL    83.54 |   110/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.42 | Train PPL    82.71 |   120/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.42 | Train PPL    82.87 |   130/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.39 | Train PPL    81.04 |   140/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.80 |   150/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.42 | Train PPL    82.83 |   160/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.38 | Train PPL    79.88 |   170/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.38 | Train PPL    79.69 |   180/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.39 | Train PPL    80.39 |   190/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.35 | Train PPL    77.78 |   200/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.43 | Train PPL    84.02 |   210/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.46 | Train PPL    86.06 |   220/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.38 | Train PPL    79.66 |   230/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.12 |   240/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.44 | Train PPL    84.58 |   250/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.39 | Train PPL    81.00 |   260/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.47 |   270/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.41 | Train PPL    82.32 |   280/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.46 |   290/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.39 | Train PPL    80.76 |   300/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.44 | Train PPL    84.67 |   310/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.35 | Train PPL    77.56 |   320/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.41 | Train PPL    82.11 |   330/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.38 | Train PPL    80.00 |   340/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.43 | Train PPL    84.23 |   350/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.41 | Train PPL    82.37 |   360/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.38 | Train PPL    79.61 |   370/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.43 | Train PPL    83.64 |   380/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.20 |   390/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.38 | Train PPL    80.08 |   400/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.86 |   410/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.35 | Train PPL    77.14 |   420/  424 Batches\n",
      "| Epoch   2 | Train Loss  4.40 | Train PPL    81.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Valid Loss  4.18 | Valid PPL    65.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   3 | Train Loss  4.80 | Train PPL   120.95 |    10/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.35 | Train PPL    77.32 |    20/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.28 | Train PPL    72.06 |    30/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.35 | Train PPL    77.33 |    40/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.30 | Train PPL    73.98 |    50/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.32 | Train PPL    75.12 |    60/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.30 | Train PPL    73.64 |    70/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.34 | Train PPL    76.56 |    80/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.29 | Train PPL    72.90 |    90/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.29 | Train PPL    72.77 |   100/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    75.81 |   110/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    75.65 |   120/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.31 | Train PPL    74.51 |   130/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.35 | Train PPL    77.26 |   140/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.32 | Train PPL    75.54 |   150/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.28 | Train PPL    72.42 |   160/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.30 | Train PPL    73.82 |   170/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.29 | Train PPL    73.06 |   180/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.32 | Train PPL    74.86 |   190/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.30 | Train PPL    73.88 |   200/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.31 | Train PPL    74.57 |   210/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.30 | Train PPL    73.78 |   220/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    75.70 |   230/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    76.09 |   240/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.34 | Train PPL    76.64 |   250/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.29 | Train PPL    72.62 |   260/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.32 | Train PPL    74.99 |   270/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.31 | Train PPL    74.21 |   280/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    75.94 |   290/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.31 | Train PPL    74.26 |   300/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.34 | Train PPL    76.33 |   310/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    76.16 |   320/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    76.31 |   330/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.29 | Train PPL    73.15 |   340/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.33 | Train PPL    76.29 |   350/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.26 | Train PPL    70.55 |   360/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.37 | Train PPL    79.12 |   370/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.31 | Train PPL    74.34 |   380/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.32 | Train PPL    74.93 |   390/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.36 | Train PPL    78.54 |   400/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.34 | Train PPL    76.80 |   410/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.35 | Train PPL    77.22 |   420/  424 Batches\n",
      "| Epoch   3 | Train Loss  4.32 | Train PPL    75.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   3 | Valid Loss  4.10 | Valid PPL    60.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   4 | Train Loss  4.67 | Train PPL   106.27 |    10/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.32 | Train PPL    75.27 |    20/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.24 | Train PPL    69.63 |    30/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.27 | Train PPL    71.64 |    40/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    70.28 |    50/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.30 | Train PPL    73.55 |    60/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.27 | Train PPL    71.51 |    70/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.27 | Train PPL    71.20 |    80/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    70.36 |    90/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.21 | Train PPL    67.21 |   100/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.24 | Train PPL    69.10 |   110/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.24 | Train PPL    69.48 |   120/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    70.07 |   130/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    69.79 |   140/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.24 | Train PPL    69.64 |   150/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.24 | Train PPL    69.66 |   160/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.24 | Train PPL    69.59 |   170/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.26 | Train PPL    70.60 |   180/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.30 | Train PPL    73.51 |   190/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.29 | Train PPL    72.76 |   200/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    70.40 |   210/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.29 | Train PPL    73.09 |   220/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    69.83 |   230/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.21 | Train PPL    67.21 |   240/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.26 | Train PPL    70.65 |   250/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.26 | Train PPL    70.87 |   260/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.29 | Train PPL    72.84 |   270/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    70.19 |   280/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    69.96 |   290/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.21 | Train PPL    67.44 |   300/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.29 | Train PPL    72.84 |   310/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    69.79 |   320/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.23 | Train PPL    68.69 |   330/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.26 | Train PPL    70.86 |   340/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.27 | Train PPL    71.56 |   350/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.28 | Train PPL    72.25 |   360/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    70.10 |   370/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.27 | Train PPL    71.54 |   380/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.22 | Train PPL    68.36 |   390/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.27 | Train PPL    71.51 |   400/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.25 | Train PPL    70.24 |   410/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.28 | Train PPL    72.04 |   420/  424 Batches\n",
      "| Epoch   4 | Train Loss  4.26 | Train PPL    70.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   4 | Valid Loss  4.06 | Valid PPL    57.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   5 | Train Loss  4.66 | Train PPL   105.87 |    10/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.22 | Train PPL    67.91 |    20/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.20 | Train PPL    66.73 |    30/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.22 | Train PPL    67.77 |    40/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.26 |    50/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.20 | Train PPL    66.40 |    60/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.27 |    70/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.19 | Train PPL    65.94 |    80/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.36 |    90/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.46 |   100/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.62 |   110/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.16 | Train PPL    64.02 |   120/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.22 | Train PPL    68.07 |   130/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.22 | Train PPL    68.23 |   140/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.19 | Train PPL    66.05 |   150/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.23 | Train PPL    68.69 |   160/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.33 |   170/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.19 | Train PPL    66.01 |   180/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.19 | Train PPL    65.85 |   190/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.19 | Train PPL    66.26 |   200/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.20 | Train PPL    66.89 |   210/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.20 | Train PPL    67.02 |   220/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.24 |   230/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.22 | Train PPL    68.08 |   240/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.24 | Train PPL    69.09 |   250/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.25 | Train PPL    70.05 |   260/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.23 | Train PPL    68.98 |   270/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.20 | Train PPL    66.36 |   280/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.22 | Train PPL    67.95 |   290/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.33 |   300/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.26 |   310/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.20 | Train PPL    66.61 |   320/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.22 | Train PPL    68.32 |   330/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.25 |   340/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.37 |   350/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.25 | Train PPL    70.03 |   360/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.18 | Train PPL    65.47 |   370/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.23 | Train PPL    68.42 |   380/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.06 |   390/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.17 | Train PPL    64.94 |   400/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.56 |   410/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.52 |   420/  424 Batches\n",
      "| Epoch   5 | Train Loss  4.21 | Train PPL    67.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   5 | Valid Loss  4.01 | Valid PPL    55.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   6 | Train Loss  4.56 | Train PPL    95.21 |    10/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.14 | Train PPL    62.99 |    20/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.18 | Train PPL    65.14 |    30/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.14 | Train PPL    62.98 |    40/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.20 | Train PPL    66.43 |    50/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.16 | Train PPL    64.04 |    60/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.12 | Train PPL    61.80 |    70/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.16 | Train PPL    64.37 |    80/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.86 |    90/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.93 |   100/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.18 | Train PPL    65.27 |   110/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.16 | Train PPL    64.00 |   120/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.15 | Train PPL    63.15 |   130/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.91 |   140/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.16 | Train PPL    63.96 |   150/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.15 | Train PPL    63.66 |   160/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.15 | Train PPL    63.21 |   170/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.77 |   180/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.94 |   190/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.20 | Train PPL    66.65 |   200/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.15 | Train PPL    63.16 |   210/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.16 | Train PPL    64.35 |   220/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.19 | Train PPL    65.78 |   230/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.15 | Train PPL    63.49 |   240/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.18 | Train PPL    65.23 |   250/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.19 | Train PPL    65.97 |   260/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.19 | Train PPL    66.30 |   270/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.15 | Train PPL    63.36 |   280/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.15 | Train PPL    63.16 |   290/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.19 | Train PPL    66.21 |   300/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.18 | Train PPL    65.63 |   310/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.20 | Train PPL    66.41 |   320/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.19 | Train PPL    65.88 |   330/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.18 | Train PPL    65.41 |   340/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.14 | Train PPL    62.63 |   350/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.50 |   360/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.19 | Train PPL    65.71 |   370/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.24 | Train PPL    69.20 |   380/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    65.03 |   390/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.45 |   400/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.18 | Train PPL    65.14 |   410/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.19 | Train PPL    65.72 |   420/  424 Batches\n",
      "| Epoch   6 | Train Loss  4.17 | Train PPL    64.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   6 | Valid Loss  3.97 | Valid PPL    53.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   7 | Train Loss  4.53 | Train PPL    92.61 |    10/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.58 |    20/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.80 |    30/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.11 | Train PPL    61.17 |    40/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.15 | Train PPL    63.43 |    50/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.15 | Train PPL    63.64 |    60/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.11 | Train PPL    60.70 |    70/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.10 | Train PPL    60.35 |    80/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.10 | Train PPL    60.61 |    90/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    62.01 |   100/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.06 | Train PPL    58.02 |   110/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    62.31 |   120/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    62.08 |   130/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.47 |   140/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.63 |   150/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.65 |   160/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    61.95 |   170/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.11 | Train PPL    60.93 |   180/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.44 |   190/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.60 |   200/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    62.24 |   210/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.16 | Train PPL    63.83 |   220/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    61.98 |   230/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    62.45 |   240/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.78 |   250/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.80 |   260/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.16 | Train PPL    64.01 |   270/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.67 |   280/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    61.97 |   290/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.58 |   300/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.19 | Train PPL    66.13 |   310/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.17 | Train PPL    64.68 |   320/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.74 |   330/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.95 |   340/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.17 | Train PPL    64.76 |   350/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    63.05 |   360/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.85 |   370/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.53 |   380/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.12 | Train PPL    61.69 |   390/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    62.48 |   400/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.14 | Train PPL    62.56 |   410/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.15 | Train PPL    63.50 |   420/  424 Batches\n",
      "| Epoch   7 | Train Loss  4.13 | Train PPL    62.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   7 | Valid Loss  3.94 | Valid PPL    51.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   8 | Train Loss  4.50 | Train PPL    90.37 |    10/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.11 | Train PPL    60.74 |    20/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.09 | Train PPL    59.80 |    30/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.03 | Train PPL    56.00 |    40/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.07 | Train PPL    58.66 |    50/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.08 | Train PPL    58.86 |    60/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.08 | Train PPL    59.25 |    70/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.07 | Train PPL    58.65 |    80/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.11 | Train PPL    61.05 |    90/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.07 | Train PPL    58.83 |   100/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.07 | Train PPL    58.46 |   110/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.34 |   120/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.08 | Train PPL    59.24 |   130/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.08 | Train PPL    59.34 |   140/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.23 |   150/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.12 | Train PPL    61.46 |   160/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.09 | Train PPL    59.53 |   170/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.09 | Train PPL    59.64 |   180/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.41 |   190/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.11 | Train PPL    61.16 |   200/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.22 |   210/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.09 | Train PPL    59.67 |   220/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.12 | Train PPL    61.81 |   230/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.20 |   240/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.13 | Train PPL    62.37 |   250/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.13 | Train PPL    62.00 |   260/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.07 | Train PPL    58.67 |   270/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.13 | Train PPL    62.12 |   280/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.11 | Train PPL    60.78 |   290/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.11 | Train PPL    61.12 |   300/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.53 |   310/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.09 | Train PPL    59.77 |   320/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.18 | Train PPL    65.29 |   330/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.14 | Train PPL    63.05 |   340/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.09 | Train PPL    59.81 |   350/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.15 | Train PPL    63.17 |   360/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.11 | Train PPL    61.22 |   370/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.14 | Train PPL    62.50 |   380/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.12 | Train PPL    61.71 |   390/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.12 | Train PPL    61.51 |   400/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.13 | Train PPL    62.30 |   410/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.48 |   420/  424 Batches\n",
      "| Epoch   8 | Train Loss  4.10 | Train PPL    60.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   8 | Valid Loss  3.93 | Valid PPL    50.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   9 | Train Loss  4.52 | Train PPL    91.45 |    10/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.07 | Train PPL    58.62 |    20/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.06 | Train PPL    57.81 |    30/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.06 | Train PPL    58.08 |    40/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.05 | Train PPL    57.63 |    50/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.05 | Train PPL    57.60 |    60/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.07 | Train PPL    58.36 |    70/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.04 | Train PPL    56.61 |    80/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.07 | Train PPL    58.50 |    90/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.09 | Train PPL    59.99 |   100/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.06 | Train PPL    58.02 |   110/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.06 | Train PPL    57.92 |   120/  424 Batches\n",
      "| Epoch   9 | Train Loss  4.07 | Train PPL    58.53 |   130/  424 Batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object tqdm_notebook.__iter__ at 0x7f579faf4138>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/roberta/miniconda3/envs/ammi/lib/python3.7/site-packages/tqdm/_tqdm_notebook.py\", line 226, in __iter__\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'sp'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-397fbbf20666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcur_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ammi-2019-nlp/01-day-LM/utils/neural_lm.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, xs, ys, eval_mode)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ammi/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ammi/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "log_interval = 10\n",
    "best_eval_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = 0   \n",
    "    cur_loss = 0\n",
    "    for i, (data, labels) in _tqdm(enumerate(train_loader), disable=True):\n",
    "        prediction, loss = model.train_step(data, labels)\n",
    "        train_loss += len(data) * loss\n",
    "        cur_loss += loss\n",
    "        \n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = cur_loss / log_interval\n",
    "            print('| Epoch {:3d} | Train Loss {:5.2f} | Train PPL {:8.2f} | {:5d}/{:5d} Batches'.format(\n",
    "                epoch, cur_loss, math.exp(cur_loss), i, int(num_train/len(data))))\n",
    "            cur_loss = 0\n",
    "    \n",
    "    train_loss = train_loss / num_train\n",
    "    print('| Epoch {:3d} | Train Loss {:5.2f} | Train PPL {:8.2f}'.format(\n",
    "            epoch, train_loss, math.exp(train_loss)))\n",
    "\n",
    "    # Eval\n",
    "    if epoch % 1 == 0:        \n",
    "        eval_loss = 0\n",
    "        for i, (data, labels) in _tqdm(enumerate(valid_loader), disable=True):\n",
    "            prediction, loss = model.train_step(data, labels, eval_mode=True)\n",
    "            eval_loss += len(data) * loss\n",
    "        eval_loss = eval_loss / num_valid \n",
    "        print('-' * 89)\n",
    "        print('| Epoch {:3d} | Valid Loss {:5.2f} | Valid PPL {:8.2f}'.format(\n",
    "            epoch, eval_loss, math.exp(eval_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_eval_loss or eval_loss < best_eval_loss:\n",
    "            with open('neural_lm_amazon_model' + '.pt', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_eval_loss = eval_loss        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sent):\n",
    "    tokenized, _ = ngram_utils.tokenize_dataset(sent)\n",
    "    sent_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "    sent_tensor = torch.LongTensor(sent_ids).to(device)\n",
    "    generated, scores = model.eval_step(sent_tensor, score_only=True)\n",
    "    ppl = math.exp(scores)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e3715b75c747d398566ab77280eb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3375035881299496"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['this is a great tutu']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c794dba285a4ed6afe9a15e269d35c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11954595367905725"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['tutu tutu is not my favorit']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4254e1458010443f9076710966a79a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2703779395546447"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['i really like this watch']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0524292e58304eea853c1c864063edce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18068775894662562"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['training neural networks']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(context=None):\n",
    "    if context is None:\n",
    "        dummy_context = torch.LongTensor([[0]]).to(device)\n",
    "        generated, scores = model.eval_step(dummy_context, use_context=False)\n",
    "    else:\n",
    "        tokenized, _ = ngram_utils.tokenize_dataset(context)\n",
    "        context_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "        context_tensor = torch.LongTensor(context_ids).to(device)\n",
    "        generated, scores = model.eval_step(context_tensor, use_context=True)\n",
    "    \n",
    "    ppl = math.exp(scores)\n",
    "    return generated, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", the quality of the the best best bra about . <eos>\n"
     ]
    }
   ],
   "source": [
    "generated, scores = generate_sentence()\n",
    "print(' '.join(word[0] for word in generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c232d14bdc49688a61a0c0f3f3a31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first , too and the size was perfect . <eos>\n"
     ]
    }
   ],
   "source": [
    "generated, scores = generate_sentence(context=['this is not my'])\n",
    "print(' '.join(word[0] for word in generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
