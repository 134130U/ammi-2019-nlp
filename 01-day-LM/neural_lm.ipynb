{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/neural_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNniJxhFMYCF"
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('utils/')\n",
    "import loading_text_and_tokenization\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.ngram_utils as ngram_utils\n",
    "from utils.ngram_utils import NgramLM\n",
    "from utils.amazon_dataset import AmazonDataset, pad, batchify\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.neural_lm import BagOfNGrams, DecoderMLP, seq2seq\n",
    "import utils.global_variables as gl\n",
    "import torch\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "_tqdm = tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f53e679a7b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .txt files and create lists of reviews\n",
    "train_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/new_train.txt', 'r') as f:\n",
    "    train_data = [review for review in f.read().split('\\n') if review]\n",
    "    \n",
    "valid_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/new_valid.txt', 'r') as f:\n",
    "    valid_data = [review for review in f.read().split('\\n') if review]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22288, 2785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"this is a great tutu and at a really great price . it doesn ' t look cheap at all . i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly . a + + \",\n",
       " list,\n",
       " 22288,\n",
       " str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], valid_data[0]\n",
    "train_data = train_data#[:100]\n",
    "valid_data = valid_data#[:10]\n",
    "train_data[0], type(train_data), len(train_data), type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c08db57ad34239807772570fc87875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3817bc9aa7cc46259a1eef0d67c4a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = ngram_utils.tokenize_dataset(train_data)\n",
    "valid_data_tokenized, all_tokens_valid = ngram_utils.tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'great',\n",
       "  'tutu',\n",
       "  'and',\n",
       "  'at',\n",
       "  'a',\n",
       "  'really',\n",
       "  'great',\n",
       "  'price',\n",
       "  '.'],\n",
       " 'this')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokenized[0], all_tokens_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_padded = ngram_utils.pad_dataset(train_data_tokenized, n=N)\n",
    "valid_data_padded = ngram_utils.pad_dataset(valid_data_tokenized, n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " '<sos>',\n",
       " '<sos>',\n",
       " '<sos>',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'tutu',\n",
       " 'and',\n",
       " 'at',\n",
       " 'a',\n",
       " 'really',\n",
       " 'great',\n",
       " 'price',\n",
       " '.',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20805, ('<sos>', '<eos>', '.', 'the', 'i', ',', 'and', 'a', 'to', \"'\"))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ngram_utils.get_vocab(train_data_padded)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20809, 20807)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token, token2id = ngram_utils.get_dict(vocab)\n",
    "len(id2token), len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ids = ngram_utils.get_ids(train_data_padded, token2id)\n",
    "valid_data_ids = ngram_utils.get_ids(valid_data_padded, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107790/107790 [00:07<00:00, 14038.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(train_data_ids, max_inp_length=None, use_cuda=True)\n",
    "train_dataset_ngrams = []\n",
    "for t in train_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        train_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "train_loader = DataLoader(train_dataset_ngrams, batch_size=2048, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15172/15172 [00:00<00:00, 32566.68it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = AmazonDataset(valid_data_ids, max_inp_length=None, use_cuda=True)\n",
    "valid_dataset_ngrams = []\n",
    "for t in valid_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        valid_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "valid_loader = DataLoader(valid_dataset_ngrams, batch_size=2048, collate_fn=batchify, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f535cc68e48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1946816, 280739)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(train_dataset_ngrams)\n",
    "num_valid = len(valid_dataset_ngrams)\n",
    "num_train, num_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfNGrams(\n",
       "  (embedding): EmbeddingBag(20809, 300, mode=mean)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = BagOfNGrams(len(id2token), emb_dim=300, hidden_size=256, out_size=128, activation='ReLU', nlayers=2, reduce='mean', dropout=0.1, batch_norm=False)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderMLP(\n",
       "  (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=20809, bias=True)\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderMLP(input_size=128, output_size=len(id2token), hidden_size=256)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): BagOfNGrams(\n",
       "    (embedding): EmbeddingBag(20809, 300, mode=mean)\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderMLP(\n",
       "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=20809, bias=True)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq2seq(encoder, decoder, id2token, use_cuda=False, lr=0.1, size_ngrams=N) \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 | Train Loss  9.80 | Train PPL 18103.76 |    10/  950 Batches\n",
      "| Epoch   0 | Train Loss  8.08 | Train PPL  3217.01 |    20/  950 Batches\n",
      "| Epoch   0 | Train Loss  7.31 | Train PPL  1495.15 |    30/  950 Batches\n",
      "| Epoch   0 | Train Loss  6.27 | Train PPL   531.09 |    40/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.75 | Train PPL   312.67 |    50/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.48 | Train PPL   239.59 |    60/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.33 | Train PPL   205.62 |    70/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.28 | Train PPL   195.95 |    80/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.14 | Train PPL   171.37 |    90/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.11 | Train PPL   166.06 |   100/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.01 | Train PPL   149.35 |   110/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.07 | Train PPL   158.69 |   120/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.88 | Train PPL   131.24 |   130/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.96 | Train PPL   142.82 |   140/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.83 | Train PPL   124.71 |   150/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.85 | Train PPL   127.85 |   160/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.83 | Train PPL   125.81 |   170/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.78 | Train PPL   119.07 |   180/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.72 | Train PPL   112.72 |   190/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.73 | Train PPL   113.15 |   200/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.69 | Train PPL   108.41 |   210/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.62 | Train PPL   101.85 |   220/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.69 | Train PPL   108.58 |   230/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.66 | Train PPL   106.16 |   240/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.70 | Train PPL   110.31 |   250/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.60 | Train PPL    99.59 |   260/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.59 | Train PPL    98.37 |   270/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.58 | Train PPL    97.74 |   280/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.56 | Train PPL    95.31 |   290/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.56 | Train PPL    95.90 |   300/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.51 | Train PPL    90.74 |   310/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.55 | Train PPL    94.98 |   320/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.54 | Train PPL    93.89 |   330/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.51 | Train PPL    90.71 |   340/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.42 | Train PPL    82.72 |   350/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.46 | Train PPL    86.58 |   360/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.44 | Train PPL    84.86 |   370/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.45 | Train PPL    86.03 |   380/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.56 |   390/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.45 | Train PPL    85.39 |   400/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.44 | Train PPL    85.06 |   410/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.42 | Train PPL    83.49 |   420/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.84 |   430/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.92 |   440/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.89 |   450/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.38 | Train PPL    79.74 |   460/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.39 | Train PPL    80.41 |   470/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.37 | Train PPL    79.29 |   480/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.36 | Train PPL    78.09 |   490/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.34 | Train PPL    76.76 |   500/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.32 | Train PPL    75.36 |   510/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.35 | Train PPL    77.70 |   520/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.33 | Train PPL    75.85 |   530/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.36 | Train PPL    78.17 |   540/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.24 |   550/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.32 | Train PPL    75.49 |   560/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.42 |   570/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.39 |   580/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.33 | Train PPL    76.15 |   590/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.28 | Train PPL    72.13 |   600/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.75 |   610/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.28 | Train PPL    72.10 |   620/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.41 |   630/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.36 | Train PPL    77.88 |   640/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    69.85 |   650/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.27 | Train PPL    71.39 |   660/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.30 | Train PPL    73.59 |   670/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.27 |   680/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.21 | Train PPL    67.20 |   690/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.61 |   700/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.12 |   710/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.23 | Train PPL    68.86 |   720/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.27 | Train PPL    71.41 |   730/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.23 | Train PPL    68.70 |   740/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    69.99 |   750/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    69.80 |   760/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.28 | Train PPL    72.13 |   770/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.27 | Train PPL    71.63 |   780/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.33 |   790/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.12 |   800/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.18 |   810/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.44 |   820/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.62 |   830/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.48 |   840/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.26 | Train PPL    70.48 |   850/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.22 | Train PPL    68.17 |   860/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.21 | Train PPL    67.44 |   870/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.26 |   880/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.18 | Train PPL    65.43 |   890/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.15 | Train PPL    63.75 |   900/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.18 | Train PPL    65.63 |   910/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.21 | Train PPL    67.30 |   920/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.06 |   930/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.22 | Train PPL    67.94 |   940/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.16 |   950/ 1601 Batches\n",
      "| Epoch   0 | Train Loss  4.61 | Train PPL   100.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   0 | Valid Loss  4.20 | Valid PPL    66.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Train Loss  4.57 | Train PPL    96.81 |    10/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.15 | Train PPL    63.58 |    20/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    61.09 |    30/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.17 | Train PPL    65.03 |    40/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.62 |    50/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.18 | Train PPL    65.31 |    60/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.67 |    70/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.15 | Train PPL    63.69 |    80/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.75 |    90/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.76 |   100/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    61.01 |   110/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.87 |   120/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.58 |   130/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.13 | Train PPL    62.24 |   140/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    63.00 |   150/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.56 |   160/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.70 |   170/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.20 | Train PPL    66.45 |   180/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    61.22 |   190/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.35 |   200/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.13 | Train PPL    62.13 |   210/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    60.03 |   220/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.70 |   230/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.66 |   240/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.87 |   250/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.16 | Train PPL    64.09 |   260/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.51 |   270/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.96 |   280/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.52 |   290/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.91 |   300/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.62 |   310/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.47 |   320/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.55 |   330/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.84 |   340/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    60.02 |   350/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.44 |   360/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.07 |   370/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.68 |   380/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.47 |   390/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.13 | Train PPL    62.33 |   400/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.77 |   410/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.65 |   420/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.00 |   430/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.74 |   440/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.06 |   450/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.84 |   460/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.33 |   470/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.05 | Train PPL    57.32 |   480/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.97 |   490/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.37 |   500/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    58.87 |   510/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.36 |   520/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.18 |   530/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.13 |   540/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.92 |   550/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.05 | Train PPL    57.18 |   560/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.01 |   570/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.06 |   580/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.05 |   590/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.04 | Train PPL    57.03 |   600/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    58.91 |   610/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.26 |   620/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.43 |   630/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.32 |   640/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.57 |   650/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.35 |   660/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.04 | Train PPL    56.68 |   670/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.18 |   680/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.86 |   690/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.55 |   700/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.18 |   710/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.24 |   720/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.37 |   730/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.13 |   740/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.87 |   750/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.03 |   760/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    57.97 |   770/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.14 |   780/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.01 | Train PPL    55.30 |   790/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.08 |   800/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.90 |   810/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.55 |   820/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.13 |   830/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.97 |   840/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.33 |   850/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.01 | Train PPL    55.22 |   860/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.64 |   870/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.71 |   880/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.05 | Train PPL    57.41 |   890/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.00 | Train PPL    54.68 |   900/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.44 |   910/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.04 | Train PPL    57.08 |   920/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.14 |   930/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.86 |   940/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.09 |   950/ 1601 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Valid Loss  4.04 | Valid PPL    56.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Train Loss  4.39 | Train PPL    80.50 |    10/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.03 | Train PPL    56.49 |    20/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.27 |    30/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.02 | Train PPL    55.45 |    40/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.78 |    50/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.37 |    60/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.41 |    70/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.28 |    80/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.00 |    90/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.03 |   100/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.39 |   110/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.45 |   120/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.19 |   130/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.23 |   140/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.05 |   150/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.56 |   160/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.97 |   170/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.79 |   180/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.84 |   190/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.43 |   200/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    54.92 |   210/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.31 |   220/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.16 |   230/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    51.73 |   240/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    50.99 |   250/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.55 |   260/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.15 |   270/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.60 |   280/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.22 |   290/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.88 |   300/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.86 |   310/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.18 |   320/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.32 |   330/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.88 |   340/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.38 |   350/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.66 |   360/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.23 |   370/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.57 |   380/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.91 |   390/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.48 |   400/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.81 |   410/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.33 |   420/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.41 |   430/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.96 |   440/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.28 |   450/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.15 |   460/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.92 | Train PPL    50.16 |   470/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.45 |   480/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.93 |   490/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.30 |   500/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.72 |   510/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.61 |   520/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    51.79 |   530/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.96 |   540/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.48 |   550/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.80 |   560/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.83 |   570/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.31 |   580/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.53 |   590/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.89 | Train PPL    48.96 |   600/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    50.79 |   610/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.45 |   620/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.32 |   630/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.67 |   640/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.14 |   650/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.69 |   660/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.28 |   670/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.09 |   680/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.58 |   690/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    51.83 |   700/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.25 |   710/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    50.90 |   720/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.63 |   730/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.36 |   740/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.63 |   750/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.07 |   760/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.76 |   770/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.72 |   780/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.82 |   790/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.84 |   800/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.94 | Train PPL    51.66 |   810/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.72 |   820/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    51.10 |   830/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.19 |   840/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.99 |   850/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    54.96 |   860/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.73 |   870/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.04 |   880/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.34 |   890/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.71 |   900/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.94 | Train PPL    51.22 |   910/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.13 |   920/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.80 |   930/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.22 |   940/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.92 | Train PPL    50.65 |   950/ 1601 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Valid Loss  3.96 | Valid PPL    52.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   3 | Train Loss  4.35 | Train PPL    77.82 |    10/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.97 | Train PPL    53.23 |    20/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.34 |    30/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.29 |    40/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.49 |    50/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.96 | Train PPL    52.35 |    60/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.82 |    70/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.95 | Train PPL    51.70 |    80/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.75 |    90/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.23 |   100/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.63 |   110/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.84 |   120/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.29 |   130/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.95 | Train PPL    51.71 |   140/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.60 |   150/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.17 |   160/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.71 |   170/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.35 |   180/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.97 | Train PPL    52.93 |   190/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.47 |   200/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.36 |   210/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.86 |   220/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.68 |   230/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.59 |   240/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.77 |   250/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.94 |   260/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.76 |   270/  950 Batches\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "log_interval = 10\n",
    "best_eval_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = 0   \n",
    "    cur_loss = 0\n",
    "    for i, (data, labels) in _tqdm(enumerate(train_loader), disable=True):\n",
    "        prediction, loss = model.train_step(data, labels)\n",
    "        train_loss += len(data) * loss\n",
    "        cur_loss += loss\n",
    "        \n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = cur_loss / log_interval\n",
    "            print('| Epoch {:3d} | Train Loss {:5.2f} | Train PPL {:8.2f} | {:5d}/{:5d} Batches'.format(\n",
    "                epoch, cur_loss, math.exp(cur_loss), i, int(num_train/len(data))))\n",
    "            cur_loss = 0\n",
    "    \n",
    "    train_loss = train_loss / num_train\n",
    "    print('| Epoch {:3d} | Train Loss {:5.2f} | Train PPL {:8.2f}'.format(\n",
    "            epoch, train_loss, math.exp(train_loss)))\n",
    "\n",
    "    # Eval\n",
    "    if epoch % 1 == 0:        \n",
    "        eval_loss = 0\n",
    "        for i, (data, labels) in _tqdm(enumerate(valid_loader), disable=True):\n",
    "            prediction, loss = model.train_step(data, labels, eval_mode=True)\n",
    "            eval_loss += len(data) * loss\n",
    "        eval_loss = eval_loss / num_valid \n",
    "        print('-' * 89)\n",
    "        print('| Epoch {:3d} | Valid Loss {:5.2f} | Valid PPL {:8.2f}'.format(\n",
    "            epoch, eval_loss, math.exp(eval_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_eval_loss or eval_loss < best_eval_loss:\n",
    "            with open('neural_lm_amazon_model' + '.pt', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_eval_loss = eval_loss        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sent):\n",
    "    import pdb; pdb.set_trace()\n",
    "    tokenized, _ = ngram_utils.tokenize_dataset(sent)\n",
    "    sent_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "    sent_tensor = torch.LongTensor(sent_ids).to(device)\n",
    "    generated, scores = model.eval_step(sent_tensor, score_only=True)\n",
    "    ppl = math.exp(scores)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['this is a great tutu']\n",
    "type(sent), len(sent), type(sent[0]), sent[0]\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(context=None):\n",
    "    import pdb; pdb.set_trace()\n",
    "    if context is None:\n",
    "        dummy_context = torch.LongTensor([[0]]).to(device)\n",
    "        generated, scores = model.eval_step(dummy_context, use_context=False)\n",
    "    else:\n",
    "        tokenized, _ = ngram_utils.tokenize_dataset(context)\n",
    "        context_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "        context_tensor = torch.LongTensor(context_ids).to(device)\n",
    "        generated, scores = model.eval_step(context_tensor, use_context=True)\n",
    "    \n",
    "    ppl = math.exp(scores)\n",
    "    return generated, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, scores = generate_sentence()\n",
    "generated_str = [' '.join(g) for g in generated]\n",
    "generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, scores = generate_sentence(context=['this is not'])\n",
    "generated_str = [' '.join(g) for g in generated]\n",
    "generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "neural_lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
