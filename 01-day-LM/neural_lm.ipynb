{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_lm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/neural_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "scOBYVciKZlX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmpKUVT3MPTd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Language Modeling"
      ]
    },
    {
      "metadata": {
        "id": "FX9VHyNyMVM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import torchtext.data as data\n",
        "\n",
        "from torchtext import vocab\n",
        "from collections import Counter\n",
        "import re\n",
        "from torchtext.data import TabularDataset "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WKEuvDcrMXIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "021b5420-feb9-4081-81a3-e7b8f2aa1e68"
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f82cac6e270>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "rjxIJqnTMao7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Language Modeling\n",
        "\n",
        "**Key Idea:** instead of using counts of n-grams, we use a neural network to estimate the probability of a word $w_i$ to follow a sequence of words $(w_{i - n + 1}, ..., w_{i - 1})$:\n",
        "\n",
        "$P(w_i|w_{i−1}, w_{i−2}, …, w_{i−n+1}) = f(\\theta)$\n",
        "\n",
        "where $f$ is a neural network parametrized by a set of parameters $\\theta$\n",
        "\n",
        "\n",
        "**Advantages of Neural LM compared to N-gram LM**\n",
        "\n",
        "1. Generalizing Ability: words with similar meanings will have embeddings that are close for some metric\n",
        "    --> mitigates the data sparsity issue in N-gram LM\n",
        "\n",
        "2. Can capture long-term dependencies"
      ]
    },
    {
      "metadata": {
        "id": "RsspusifOaXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ]
    },
    {
      "metadata": {
        "id": "sdOFgRv2OcQa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build the Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "GonULOxJOglT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define the Model"
      ]
    },
    {
      "metadata": {
        "id": "WlymJtIJOj5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define the Loss Function and Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "VrdpPm70Oj8Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ]
    },
    {
      "metadata": {
        "id": "wUgBEgsHOj-k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Model on Test Data"
      ]
    },
    {
      "metadata": {
        "id": "8IBmBeL2OuqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Examples & Analysis"
      ]
    },
    {
      "metadata": {
        "id": "ZNniJxhFMYCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}