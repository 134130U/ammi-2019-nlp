{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/neural_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNniJxhFMYCF"
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('utils/')\n",
    "import loading_text_and_tokenization\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.ngram_utils as ngram_utils\n",
    "from utils.ngram_utils import NgramLM\n",
    "from utils.amazon_dataset import AmazonDataset, pad, batchify\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.neural_lm import BagOfNGrams, DecoderMLP, seq2seq\n",
    "import utils.global_variables as gl\n",
    "import torch\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "_tqdm = tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f53e679a7b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .txt files and create lists of reviews\n",
    "train_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/new_train.txt', 'r') as f:\n",
    "    train_data = [review for review in f.read().split('\\n') if review]\n",
    "    \n",
    "valid_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/new_valid.txt', 'r') as f:\n",
    "    valid_data = [review for review in f.read().split('\\n') if review]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22288, 2785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"this is a great tutu and at a really great price . it doesn ' t look cheap at all . i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly . a + + \",\n",
       " list,\n",
       " 22288,\n",
       " str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], valid_data[0]\n",
    "train_data = train_data#[:100]\n",
    "valid_data = valid_data#[:10]\n",
    "train_data[0], type(train_data), len(train_data), type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c08db57ad34239807772570fc87875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3817bc9aa7cc46259a1eef0d67c4a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = ngram_utils.tokenize_dataset(train_data)\n",
    "valid_data_tokenized, all_tokens_valid = ngram_utils.tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'great',\n",
       "  'tutu',\n",
       "  'and',\n",
       "  'at',\n",
       "  'a',\n",
       "  'really',\n",
       "  'great',\n",
       "  'price',\n",
       "  '.'],\n",
       " 'this')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokenized[0], all_tokens_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_padded = ngram_utils.pad_dataset(train_data_tokenized, n=N)\n",
    "valid_data_padded = ngram_utils.pad_dataset(valid_data_tokenized, n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " '<sos>',\n",
       " '<sos>',\n",
       " '<sos>',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'tutu',\n",
       " 'and',\n",
       " 'at',\n",
       " 'a',\n",
       " 'really',\n",
       " 'great',\n",
       " 'price',\n",
       " '.',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20805, ('<sos>', '<eos>', '.', 'the', 'i', ',', 'and', 'a', 'to', \"'\"))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ngram_utils.get_vocab(train_data_padded)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20809, 20807)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token, token2id = ngram_utils.get_dict(vocab)\n",
    "len(id2token), len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ids = ngram_utils.get_ids(train_data_padded, token2id)\n",
    "valid_data_ids = ngram_utils.get_ids(valid_data_padded, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107790/107790 [00:07<00:00, 14038.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(train_data_ids, max_inp_length=None, use_cuda=True)\n",
    "train_dataset_ngrams = []\n",
    "for t in train_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        train_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "train_loader = DataLoader(train_dataset_ngrams, batch_size=2048, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15172/15172 [00:00<00:00, 32566.68it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = AmazonDataset(valid_data_ids, max_inp_length=None, use_cuda=True)\n",
    "valid_dataset_ngrams = []\n",
    "for t in valid_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        valid_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "valid_loader = DataLoader(valid_dataset_ngrams, batch_size=2048, collate_fn=batchify, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f535cc68e48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1946816, 280739)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(train_dataset_ngrams)\n",
    "num_valid = len(valid_dataset_ngrams)\n",
    "num_train, num_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfNGrams(\n",
       "  (embedding): EmbeddingBag(20809, 300, mode=mean)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = BagOfNGrams(len(id2token), emb_dim=300, hidden_size=256, out_size=128, activation='ReLU', nlayers=2, reduce='mean', dropout=0.1, batch_norm=False)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderMLP(\n",
       "  (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=20809, bias=True)\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderMLP(input_size=128, output_size=len(id2token), hidden_size=256)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): BagOfNGrams(\n",
       "    (embedding): EmbeddingBag(20809, 300, mode=mean)\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderMLP(\n",
       "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=20809, bias=True)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq2seq(encoder, decoder, id2token, use_cuda=False, lr=0.1, size_ngrams=N) \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 | Train Loss  9.80 | Train PPL 18103.76 |    10/  950 Batches\n",
      "| Epoch   0 | Train Loss  8.08 | Train PPL  3217.01 |    20/  950 Batches\n",
      "| Epoch   0 | Train Loss  7.31 | Train PPL  1495.15 |    30/  950 Batches\n",
      "| Epoch   0 | Train Loss  6.27 | Train PPL   531.09 |    40/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.75 | Train PPL   312.67 |    50/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.48 | Train PPL   239.59 |    60/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.33 | Train PPL   205.62 |    70/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.28 | Train PPL   195.95 |    80/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.14 | Train PPL   171.37 |    90/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.11 | Train PPL   166.06 |   100/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.01 | Train PPL   149.35 |   110/  950 Batches\n",
      "| Epoch   0 | Train Loss  5.07 | Train PPL   158.69 |   120/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.88 | Train PPL   131.24 |   130/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.96 | Train PPL   142.82 |   140/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.83 | Train PPL   124.71 |   150/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.85 | Train PPL   127.85 |   160/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.83 | Train PPL   125.81 |   170/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.78 | Train PPL   119.07 |   180/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.72 | Train PPL   112.72 |   190/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.73 | Train PPL   113.15 |   200/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.69 | Train PPL   108.41 |   210/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.62 | Train PPL   101.85 |   220/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.69 | Train PPL   108.58 |   230/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.66 | Train PPL   106.16 |   240/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.70 | Train PPL   110.31 |   250/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.60 | Train PPL    99.59 |   260/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.59 | Train PPL    98.37 |   270/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.58 | Train PPL    97.74 |   280/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.56 | Train PPL    95.31 |   290/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.56 | Train PPL    95.90 |   300/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.51 | Train PPL    90.74 |   310/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.55 | Train PPL    94.98 |   320/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.54 | Train PPL    93.89 |   330/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.51 | Train PPL    90.71 |   340/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.42 | Train PPL    82.72 |   350/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.46 | Train PPL    86.58 |   360/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.44 | Train PPL    84.86 |   370/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.45 | Train PPL    86.03 |   380/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.56 |   390/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.45 | Train PPL    85.39 |   400/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.44 | Train PPL    85.06 |   410/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.42 | Train PPL    83.49 |   420/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.84 |   430/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.92 |   440/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.43 | Train PPL    83.89 |   450/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.38 | Train PPL    79.74 |   460/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.39 | Train PPL    80.41 |   470/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.37 | Train PPL    79.29 |   480/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.36 | Train PPL    78.09 |   490/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.34 | Train PPL    76.76 |   500/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.32 | Train PPL    75.36 |   510/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.35 | Train PPL    77.70 |   520/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.33 | Train PPL    75.85 |   530/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.36 | Train PPL    78.17 |   540/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.24 |   550/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.32 | Train PPL    75.49 |   560/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.42 |   570/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.39 |   580/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.33 | Train PPL    76.15 |   590/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.28 | Train PPL    72.13 |   600/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.31 | Train PPL    74.75 |   610/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.28 | Train PPL    72.10 |   620/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.41 |   630/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.36 | Train PPL    77.88 |   640/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    69.85 |   650/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.27 | Train PPL    71.39 |   660/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.30 | Train PPL    73.59 |   670/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.27 |   680/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.21 | Train PPL    67.20 |   690/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.61 |   700/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.12 |   710/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.23 | Train PPL    68.86 |   720/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.27 | Train PPL    71.41 |   730/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.23 | Train PPL    68.70 |   740/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    69.99 |   750/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    69.80 |   760/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.28 | Train PPL    72.13 |   770/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.27 | Train PPL    71.63 |   780/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.33 |   790/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.12 |   800/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.18 |   810/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.25 | Train PPL    70.44 |   820/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.62 |   830/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.24 | Train PPL    69.48 |   840/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.26 | Train PPL    70.48 |   850/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.22 | Train PPL    68.17 |   860/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.21 | Train PPL    67.44 |   870/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.26 |   880/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.18 | Train PPL    65.43 |   890/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.15 | Train PPL    63.75 |   900/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.18 | Train PPL    65.63 |   910/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.21 | Train PPL    67.30 |   920/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.06 |   930/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.22 | Train PPL    67.94 |   940/  950 Batches\n",
      "| Epoch   0 | Train Loss  4.19 | Train PPL    66.16 |   950/ 1601 Batches\n",
      "| Epoch   0 | Train Loss  4.61 | Train PPL   100.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   0 | Valid Loss  4.20 | Valid PPL    66.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Train Loss  4.57 | Train PPL    96.81 |    10/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.15 | Train PPL    63.58 |    20/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    61.09 |    30/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.17 | Train PPL    65.03 |    40/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.62 |    50/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.18 | Train PPL    65.31 |    60/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.67 |    70/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.15 | Train PPL    63.69 |    80/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.75 |    90/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.76 |   100/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    61.01 |   110/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.87 |   120/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.58 |   130/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.13 | Train PPL    62.24 |   140/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    63.00 |   150/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.56 |   160/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.70 |   170/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.20 | Train PPL    66.45 |   180/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    61.22 |   190/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.35 |   200/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.13 | Train PPL    62.13 |   210/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    60.03 |   220/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.70 |   230/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.66 |   240/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.87 |   250/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.16 | Train PPL    64.09 |   260/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.51 |   270/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.14 | Train PPL    62.96 |   280/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.52 |   290/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.91 |   300/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.62 |   310/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.47 |   320/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.55 |   330/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.84 |   340/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    60.02 |   350/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.44 |   360/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.07 |   370/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.68 |   380/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.47 |   390/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.13 | Train PPL    62.33 |   400/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.77 |   410/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.12 | Train PPL    61.65 |   420/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.00 |   430/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.74 |   440/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.06 |   450/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.84 |   460/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.33 |   470/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.05 | Train PPL    57.32 |   480/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.97 |   490/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.37 |   500/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    58.87 |   510/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.36 |   520/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.18 |   530/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.13 |   540/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.11 | Train PPL    60.92 |   550/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.05 | Train PPL    57.18 |   560/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.01 |   570/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.06 |   580/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.05 |   590/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.04 | Train PPL    57.03 |   600/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    58.91 |   610/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.26 |   620/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.43 |   630/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.32 |   640/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.57 |   650/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.35 |   660/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.04 | Train PPL    56.68 |   670/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.18 |   680/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.86 |   690/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.55 |   700/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.08 | Train PPL    59.18 |   710/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.24 |   720/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.37 |   730/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.13 |   740/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.87 |   750/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.03 |   760/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    57.97 |   770/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.14 |   780/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.01 | Train PPL    55.30 |   790/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.08 |   800/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.90 |   810/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.55 |   820/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.13 |   830/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.97 |   840/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.10 | Train PPL    60.33 |   850/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.01 | Train PPL    55.22 |   860/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.64 |   870/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.07 | Train PPL    58.71 |   880/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.05 | Train PPL    57.41 |   890/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.00 | Train PPL    54.68 |   900/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.44 |   910/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.04 | Train PPL    57.08 |   920/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.06 | Train PPL    58.14 |   930/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.02 | Train PPL    55.86 |   940/  950 Batches\n",
      "| Epoch   1 | Train Loss  4.03 | Train PPL    56.09 |   950/ 1601 Batches\n",
      "| Epoch   1 | Train Loss  4.09 | Train PPL    59.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Valid Loss  4.04 | Valid PPL    56.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Train Loss  4.39 | Train PPL    80.50 |    10/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.03 | Train PPL    56.49 |    20/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.27 |    30/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.02 | Train PPL    55.45 |    40/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.78 |    50/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.37 |    60/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.41 |    70/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.28 |    80/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.00 |    90/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.03 |   100/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    55.39 |   110/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.45 |   120/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.19 |   130/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.23 |   140/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.05 |   150/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.56 |   160/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.97 |   170/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.79 |   180/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.84 |   190/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.43 |   200/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    54.92 |   210/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.31 |   220/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.16 |   230/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    51.73 |   240/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    50.99 |   250/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.55 |   260/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.15 |   270/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.60 |   280/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.22 |   290/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.88 |   300/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.86 |   310/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.18 |   320/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.32 |   330/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.88 |   340/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.38 |   350/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.66 |   360/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.23 |   370/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.57 |   380/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.91 |   390/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.48 |   400/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.81 |   410/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.33 |   420/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.41 |   430/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.96 |   440/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.28 |   450/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.15 |   460/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.92 | Train PPL    50.16 |   470/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.45 |   480/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.93 |   490/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.30 |   500/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.72 |   510/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.61 |   520/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    51.79 |   530/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.96 |   540/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.48 |   550/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.80 |   560/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.83 |   570/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.31 |   580/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.53 |   590/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.89 | Train PPL    48.96 |   600/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    50.79 |   610/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.45 |   620/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.32 |   630/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.67 |   640/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.14 |   650/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.69 |   660/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.28 |   670/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.09 |   680/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.58 |   690/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    51.83 |   700/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.25 |   710/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    50.90 |   720/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.63 |   730/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.36 |   740/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.63 |   750/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.07 |   760/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.76 |   770/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.72 |   780/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.00 | Train PPL    54.82 |   790/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    53.84 |   800/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.94 | Train PPL    51.66 |   810/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.72 |   820/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.93 | Train PPL    51.10 |   830/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.99 | Train PPL    54.19 |   840/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.99 |   850/  950 Batches\n",
      "| Epoch   2 | Train Loss  4.01 | Train PPL    54.96 |   860/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.73 |   870/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.04 |   880/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.34 |   890/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.96 | Train PPL    52.71 |   900/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.94 | Train PPL    51.22 |   910/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.95 | Train PPL    52.13 |   920/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    52.80 |   930/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.97 | Train PPL    53.22 |   940/  950 Batches\n",
      "| Epoch   2 | Train Loss  3.92 | Train PPL    50.65 |   950/ 1601 Batches\n",
      "| Epoch   2 | Train Loss  3.98 | Train PPL    53.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Valid Loss  3.96 | Valid PPL    52.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   3 | Train Loss  4.35 | Train PPL    77.82 |    10/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.97 | Train PPL    53.23 |    20/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.34 |    30/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.29 |    40/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.49 |    50/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.96 | Train PPL    52.35 |    60/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.82 |    70/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.95 | Train PPL    51.70 |    80/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.75 |    90/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.23 |   100/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.63 |   110/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.84 |   120/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.29 |   130/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.95 | Train PPL    51.71 |   140/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.60 |   150/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.17 |   160/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.71 |   170/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.35 |   180/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.97 | Train PPL    52.93 |   190/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.47 |   200/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.36 |   210/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.86 |   220/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.68 |   230/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.59 |   240/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.77 |   250/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.94 |   260/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.76 |   270/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.29 |   280/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.32 |   290/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    50.10 |   300/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    50.03 |   310/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.46 |   320/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.47 |   330/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.22 |   340/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.51 |   350/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.31 |   360/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    50.09 |   370/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.96 |   380/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.59 |   390/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.27 |   400/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    49.04 |   410/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.85 |   420/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.45 |   430/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.17 |   440/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    51.08 |   450/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.48 |   460/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.95 |   470/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.75 |   480/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.41 |   490/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.34 |   500/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.94 | Train PPL    51.37 |   510/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.25 |   520/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    51.05 |   530/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.40 |   540/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.44 |   550/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.90 |   560/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.81 |   570/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.51 |   580/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.23 |   590/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.95 | Train PPL    52.02 |   600/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    50.04 |   610/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.86 | Train PPL    47.60 |   620/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.72 |   630/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.95 |   640/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    51.10 |   650/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.86 | Train PPL    47.59 |   660/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.39 |   670/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.88 | Train PPL    48.26 |   680/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.96 |   690/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.96 |   700/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.75 |   710/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.52 |   720/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.79 |   730/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.84 |   740/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.98 |   750/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.32 |   760/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.86 |   770/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.76 |   780/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.25 |   790/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.91 |   800/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.75 |   810/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.35 |   820/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    47.88 |   830/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.75 |   840/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.76 |   850/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.86 | Train PPL    47.51 |   860/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.74 |   870/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    48.05 |   880/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.87 | Train PPL    48.07 |   890/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.92 | Train PPL    50.46 |   900/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.93 | Train PPL    50.74 |   910/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.89 | Train PPL    48.79 |   920/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.43 |   930/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.90 | Train PPL    49.52 |   940/  950 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.67 |   950/ 1601 Batches\n",
      "| Epoch   3 | Train Loss  3.91 | Train PPL    49.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   3 | Valid Loss  3.90 | Valid PPL    49.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   4 | Train Loss  4.30 | Train PPL    73.63 |    10/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.30 |    20/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.58 |    30/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.64 |    40/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    48.04 |    50/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    48.01 |    60/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.29 |    70/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.99 |    80/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.81 | Train PPL    45.25 |    90/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    49.00 |   100/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.81 |   110/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.43 |   120/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.33 |   130/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.49 |   140/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    48.08 |   150/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.00 |   160/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.47 |   170/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.99 |   180/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.25 |   190/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.90 | Train PPL    49.28 |   200/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    49.02 |   210/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.86 |   220/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.48 |   230/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    46.77 |   240/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.29 |   250/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.09 |   260/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.91 | Train PPL    49.99 |   270/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.88 |   280/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.38 |   290/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.91 | Train PPL    49.89 |   300/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.72 |   310/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.82 | Train PPL    45.62 |   320/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.55 |   330/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.77 |   340/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.14 |   350/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.05 |   360/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    46.21 |   370/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.56 |   380/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.59 |   390/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.65 |   400/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.55 |   410/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.73 |   420/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.41 |   430/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.33 |   440/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.76 |   450/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.44 |   460/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    46.25 |   470/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    46.02 |   480/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.30 |   490/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.87 |   500/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.37 |   510/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.85 |   520/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.89 |   530/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    46.19 |   540/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.69 |   550/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.57 |   560/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    45.99 |   570/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    48.10 |   580/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.91 |   590/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.74 |   600/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.68 |   610/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.56 |   620/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    46.03 |   630/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.48 |   640/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.75 |   650/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.05 |   660/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.23 |   670/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.86 |   680/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.64 |   690/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.32 |   700/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.87 |   710/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    45.89 |   720/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.37 |   730/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.32 |   740/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.51 |   750/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.44 |   760/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.24 |   770/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.71 |   780/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.08 |   790/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.93 |   800/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.87 | Train PPL    47.79 |   810/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.84 | Train PPL    46.54 |   820/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.71 |   830/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.89 | Train PPL    48.89 |   840/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.57 |   850/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.07 |   860/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.36 |   870/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    46.78 |   880/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    46.11 |   890/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.82 | Train PPL    45.75 |   900/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.82 | Train PPL    45.40 |   910/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    47.16 |   920/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.88 | Train PPL    48.34 |   930/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.83 | Train PPL    45.94 |   940/  950 Batches\n",
      "| Epoch   4 | Train Loss  3.85 | Train PPL    46.77 |   950/ 1601 Batches\n",
      "| Epoch   4 | Train Loss  3.86 | Train PPL    47.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   4 | Valid Loss  3.86 | Valid PPL    47.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   5 | Train Loss  4.28 | Train PPL    71.89 |    10/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.86 | Train PPL    47.54 |    20/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.84 | Train PPL    46.62 |    30/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.82 | Train PPL    45.39 |    40/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.80 | Train PPL    44.62 |    50/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.84 | Train PPL    46.70 |    60/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.87 | Train PPL    47.76 |    70/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.85 | Train PPL    47.07 |    80/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.84 | Train PPL    46.43 |    90/  950 Batches\n",
      "| Epoch   5 | Train Loss  3.82 | Train PPL    45.70 |   100/  950 Batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object tqdm_notebook.__iter__ at 0x7f532f10ccf0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/roberta/miniconda3/envs/ammi/lib/python3.7/site-packages/tqdm/_tqdm_notebook.py\", line 226, in __iter__\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'sp'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-397fbbf20666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcur_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ammi-2019-nlp/01-day-LM/utils/neural_lm.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, xs, ys, eval_mode)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mbow_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ammi/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ammi-2019-nlp/01-day-LM/utils/neural_lm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"\"\"\n\u001b[1;32m     70\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ammi/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ammi/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ammi/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "log_interval = 10\n",
    "best_eval_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = 0   \n",
    "    cur_loss = 0\n",
    "    for i, (data, labels) in _tqdm(enumerate(train_loader), disable=True):\n",
    "        prediction, loss = model.train_step(data, labels)\n",
    "        train_loss += len(data) * loss\n",
    "        cur_loss += loss\n",
    "        \n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = cur_loss / log_interval\n",
    "            print('| Epoch {:3d} | Train Loss {:5.2f} | Train PPL {:8.2f} | {:5d}/{:5d} Batches'.format(\n",
    "                epoch, cur_loss, math.exp(cur_loss), i, int(num_train/len(data))))\n",
    "            cur_loss = 0\n",
    "    \n",
    "    train_loss = train_loss / num_train\n",
    "    print('| Epoch {:3d} | Train Loss {:5.2f} | Train PPL {:8.2f}'.format(\n",
    "            epoch, train_loss, math.exp(train_loss)))\n",
    "\n",
    "    # Eval\n",
    "    if epoch % 1 == 0:        \n",
    "        eval_loss = 0\n",
    "        for i, (data, labels) in _tqdm(enumerate(valid_loader), disable=True):\n",
    "            prediction, loss = model.train_step(data, labels, eval_mode=True)\n",
    "            eval_loss += len(data) * loss\n",
    "        eval_loss = eval_loss / num_valid \n",
    "        print('-' * 89)\n",
    "        print('| Epoch {:3d} | Valid Loss {:5.2f} | Valid PPL {:8.2f}'.format(\n",
    "            epoch, eval_loss, math.exp(eval_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_eval_loss or eval_loss < best_eval_loss:\n",
    "            with open('neural_lm_amazon_model' + '.pt', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_eval_loss = eval_loss        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sent):\n",
    "    tokenized, _ = ngram_utils.tokenize_dataset(sent)\n",
    "    sent_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "    sent_tensor = torch.LongTensor(sent_ids).to(device)\n",
    "    generated, scores = model.eval_step(sent_tensor, score_only=True)\n",
    "    ppl = math.exp(scores)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1eb948a25949a6aa8bc28849bea3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1470156052928668"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['this is a great tutu']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0219e600b614db29e3cd2f77a533ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11621235433872693"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['i do not like this tutu']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b47b46139a431c97a3bc6d02ef92f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10470419285152689"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['i will request a refund']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e039e41c9148e9b41ad87443ab047a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09637728946146001"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['i do not understand']\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a3a0ee46dd404e8c02e3d251f7c25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1783618638572311"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['this is a very cool watch .']\n",
    "type(sent), len(sent), type(sent[0]), sent[0]\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3dcf87c5c7465897c3019351c308b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07417229267866518"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['blah watch blah what']\n",
    "type(sent), len(sent), type(sent[0]), sent[0]\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"these are not sized right . a 3x is always big on me and these r cut wrong ! i ' m returning them . i think they are darling but they are just cut wrong . i got them in a timely fashion . there was no problem with the seller . just an issue with sizing . \",\n",
       " 'a cool watch to have ',\n",
       " \"i own several watches , and this is my favorite dress watch . had a breitling before , but it ' s high profile didn ' t work well with my dress shirts . the slim design of the bulova makes this clearly a classic dress watch . \",\n",
       " \"i am happy with my recent purchase of the bulova men ' s 97b13 black tortoise shell leather strap watch . it was my first purchase of a bulova watch , and is amazed by the size and feel of my watch , \",\n",
       " 'having read hundreds of reviews of watches on this website i chose this one to add to my collection : - likes : - an elegant and easy to set watch - utterly easy to use and simpledislikes - perhaps too small for anyone with big wrists , the wrist band does not feel like it is high quality ',\n",
       " 'this is a favorite timepiece . the classic roman numbers on the clean white dial and contrasting black hands make this an all the time watch . i have a breitling navitimer and a vacheron , but the bulova goes everywhere . the calendar is handy on the job . the quartz movement is right on the mark when checked with wwv or the internet site . the leather strap is the right choice for the watch and it completes the classic look and style . i would buy it again and the price is a value . i like the quartz movt . as it is accurate and does not stop during a long rest . the watchcase is nicely styled with the lugs helping the classic look . the crown is small and causes no skin irratation , the calander is quick set and the gold second hand does not get confused with the minute hand . it is winner ! ',\n",
       " 'this watch looked good in the pictures , the dial is slim , and it is perfectly adequate , but the strap is short and looked cheap upon arrival . not a bad watch , but not great . i ended up returning mine . i would recommend viewing this model in person before buying to make sure you like it . thank god amazon makes returns easy . ',\n",
       " \"i definitely have loved what bulova has made in watches the past several years . not just the classic design , but how they have stood by the customers on what has made the customers keeping with time . this one is definitely the case with this watch . this brand new bulova men ' s leather watch is definitely a classic and simple design . it definitely improves on what bulova has had before with their basic leather - strapped watches . i definitely recommend this piece a lot , if you are looking for a basic piece that is classic , affordable , and a real steal for anyone who wants a watch that is definitely worth while in a beautiful quartz movement . this bulova watch is definitely worth the money . price : bsetup & quality : b - overall : b 1 / 2 - \",\n",
       " \"this is my ideal in an elegant watch - - gold ( tone ) case , uncluttered white face , black roman numerals . i have a small wrist and it fits perfectly . yes , the date window is a bit small , but i ' ve found that i rarely need to check the date . \",\n",
       " \"bought 6 of these for groomsman gift . something i new they wouldn ' t have but every gentleman should have . a dress watch . the only thing i didn ' t like about the watch was the small cheap wrist - band . my wife bought me a silver bulove same style as wedding gift and the band was top notch on mine . the band that comes with these will need to be replaced but the price is great ! \"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(context=None):\n",
    "    if context is None:\n",
    "        dummy_context = torch.LongTensor([[0]]).to(device)\n",
    "        generated, scores = model.eval_step(dummy_context, use_context=False)\n",
    "    else:\n",
    "        tokenized, _ = ngram_utils.tokenize_dataset(context)\n",
    "        context_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "        context_tensor = torch.LongTensor(context_ids).to(device)\n",
    "        generated, scores = model.eval_step(context_tensor, use_context=True)\n",
    "    \n",
    "    ppl = math.exp(scores)\n",
    "    return generated, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the only reason i wish that they were made of a little the little the back . <eos>\n"
     ]
    }
   ],
   "source": [
    "generated, scores = generate_sentence()\n",
    "print(' '.join(word[0] for word in generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabaa6de8b0845a391ee45066e884eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: out of range at /opt/conda/conda-bld/pytorch_1549633347309/work/aten/src/TH/generic/THTensor.cpp:350",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-32dad6a8e6da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'this is not my'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-d85053e66cb1>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcontext_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcontext_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ammi-2019-nlp/01-day-LM/utils/neural_lm.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(self, xs, use_context, score_only)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mprev_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: out of range at /opt/conda/conda-bld/pytorch_1549633347309/work/aten/src/TH/generic/THTensor.cpp:350"
     ]
    }
   ],
   "source": [
    "generated, scores = generate_sentence(context=['this is not my'])\n",
    "print(' '.join(word[0] for word in generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "neural_lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
