from torchtext.data import TabularDataset 
from torch.utils.data import Dataset, DataLoader

class AmazonDataset(Dataset):
    def __init__(self, data_list, max_inp_length=None, use_cuda=True):
        """
        data_list is a list of tuples: (x,y) where x is a list of ids and y is a label
        """
        self.data = data_list
        self.max_len = max_inp_length
        self.data_tensors = []
        device = torch.device("cuda" if (torch.cuda.is_available() and use_cuda) else "cpu")
        for (i, t) in tqdm_notebook(self.data):
            print(i, t)
            self.data_tensors.append((torch.LongTensor(i[:self.max_len]).to(device), \
                                        torch.LongTensor([t]).to(device)))
                
    def __getitem__(self, key):
        (inp, tgt) = self.data_tensors[key]
        
        return inp, tgt, len(inp)

    def __len__(self):
        return len(self.data)

    def pad(tensor, length, dim=0, pad=0):
        """Pad tensor to a specific length.
        :param tensor: vector to pad
        :param length: new length
        :param dim: (default 0) dimension to pad
        :returns: padded tensor if the tensor is shorter than length
        """
        if tensor.size(dim) < length:
            return torch.cat(
                [tensor, tensor.new(*tensor.size()[:dim],
                                    length - tensor.size(dim),
                                    *tensor.size()[dim + 1:]).fill_(pad)],
                dim=dim)
        else:
            return tensor

    def batchify(batch):
        maxlen = max(batch, key = itemgetter(2))[-1]
        batch_list = []
        target_list = []
        for b in batch:
            batch_list.append(pad(b[0], maxlen, dim=0, pad=PAD_IDX))
            target_list.append(b[1])
        input_batch = torch.stack(batch_list, 0)
        target_batch = torch.stack(target_list, 0)

        return input_batch, target_batch